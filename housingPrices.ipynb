{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('datasets/housing/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('datasets/housing/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 80)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train data mining (fillna, drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 81 columns):\n",
      "Id               1460 non-null int64\n",
      "MSSubClass       1460 non-null int64\n",
      "MSZoning         1460 non-null object\n",
      "LotFrontage      1201 non-null float64\n",
      "LotArea          1460 non-null int64\n",
      "Street           1460 non-null object\n",
      "Alley            91 non-null object\n",
      "LotShape         1460 non-null object\n",
      "LandContour      1460 non-null object\n",
      "Utilities        1460 non-null object\n",
      "LotConfig        1460 non-null object\n",
      "LandSlope        1460 non-null object\n",
      "Neighborhood     1460 non-null object\n",
      "Condition1       1460 non-null object\n",
      "Condition2       1460 non-null object\n",
      "BldgType         1460 non-null object\n",
      "HouseStyle       1460 non-null object\n",
      "OverallQual      1460 non-null int64\n",
      "OverallCond      1460 non-null int64\n",
      "YearBuilt        1460 non-null int64\n",
      "YearRemodAdd     1460 non-null int64\n",
      "RoofStyle        1460 non-null object\n",
      "RoofMatl         1460 non-null object\n",
      "Exterior1st      1460 non-null object\n",
      "Exterior2nd      1460 non-null object\n",
      "MasVnrType       1452 non-null object\n",
      "MasVnrArea       1452 non-null float64\n",
      "ExterQual        1460 non-null object\n",
      "ExterCond        1460 non-null object\n",
      "Foundation       1460 non-null object\n",
      "BsmtQual         1423 non-null object\n",
      "BsmtCond         1423 non-null object\n",
      "BsmtExposure     1422 non-null object\n",
      "BsmtFinType1     1423 non-null object\n",
      "BsmtFinSF1       1460 non-null int64\n",
      "BsmtFinType2     1422 non-null object\n",
      "BsmtFinSF2       1460 non-null int64\n",
      "BsmtUnfSF        1460 non-null int64\n",
      "TotalBsmtSF      1460 non-null int64\n",
      "Heating          1460 non-null object\n",
      "HeatingQC        1460 non-null object\n",
      "CentralAir       1460 non-null object\n",
      "Electrical       1459 non-null object\n",
      "1stFlrSF         1460 non-null int64\n",
      "2ndFlrSF         1460 non-null int64\n",
      "LowQualFinSF     1460 non-null int64\n",
      "GrLivArea        1460 non-null int64\n",
      "BsmtFullBath     1460 non-null int64\n",
      "BsmtHalfBath     1460 non-null int64\n",
      "FullBath         1460 non-null int64\n",
      "HalfBath         1460 non-null int64\n",
      "BedroomAbvGr     1460 non-null int64\n",
      "KitchenAbvGr     1460 non-null int64\n",
      "KitchenQual      1460 non-null object\n",
      "TotRmsAbvGrd     1460 non-null int64\n",
      "Functional       1460 non-null object\n",
      "Fireplaces       1460 non-null int64\n",
      "FireplaceQu      770 non-null object\n",
      "GarageType       1379 non-null object\n",
      "GarageYrBlt      1379 non-null float64\n",
      "GarageFinish     1379 non-null object\n",
      "GarageCars       1460 non-null int64\n",
      "GarageArea       1460 non-null int64\n",
      "GarageQual       1379 non-null object\n",
      "GarageCond       1379 non-null object\n",
      "PavedDrive       1460 non-null object\n",
      "WoodDeckSF       1460 non-null int64\n",
      "OpenPorchSF      1460 non-null int64\n",
      "EnclosedPorch    1460 non-null int64\n",
      "3SsnPorch        1460 non-null int64\n",
      "ScreenPorch      1460 non-null int64\n",
      "PoolArea         1460 non-null int64\n",
      "PoolQC           7 non-null object\n",
      "Fence            281 non-null object\n",
      "MiscFeature      54 non-null object\n",
      "MiscVal          1460 non-null int64\n",
      "MoSold           1460 non-null int64\n",
      "YrSold           1460 non-null int64\n",
      "SaleType         1460 non-null object\n",
      "SaleCondition    1460 non-null object\n",
      "SalePrice        1460 non-null int64\n",
      "dtypes: float64(3), int64(35), object(43)\n",
      "memory usage: 924.0+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['LotFrontage'] = train['LotFrontage'].fillna(train['LotFrontage'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['MasVnrType'] = train['MasVnrType'].fillna(train['MasVnrType'].mode()[0])\n",
    "train['MasVnrArea'] = train['MasVnrArea'].fillna(train['MasVnrArea'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['BsmtCond'] = train['BsmtCond'].fillna(train['BsmtCond'].mode()[0])\n",
    "train['BsmtQual'] = train['BsmtQual'].fillna(train['BsmtQual'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['BsmtExposure'] = train['BsmtExposure'].fillna(train['BsmtExposure'].mode()[0])\n",
    "train['BsmtFinType1'] = train['BsmtFinType1'].fillna(train['BsmtFinType1'].mode()[0])\n",
    "train['BsmtFinType2'] = train['BsmtFinType2'].fillna(train['BsmtFinType2'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Electrical'] = train['Electrical'].fillna(train['Electrical'].mode()[0])\n",
    "train['FireplaceQu'] = train['FireplaceQu'].fillna(train['FireplaceQu'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['GarageType'] = train['GarageType'].fillna(train['GarageType'].mode()[0])\n",
    "train['GarageFinish'] = train['GarageFinish'].fillna(train['GarageFinish'].mode()[0])\n",
    "train['GarageQual'] = train['GarageQual'].fillna(train['GarageQual'].mode()[0])\n",
    "train['GarageCond'] = train['GarageCond'].fillna(train['GarageCond'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['Alley'],axis=1,inplace=True)\n",
    "train.drop(['GarageYrBlt'],axis=1,inplace=True)\n",
    "train.drop(['PoolQC'],axis=1,inplace=True)\n",
    "train.drop(['Fence'],axis=1,inplace=True)\n",
    "train.drop(['MiscFeature'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['Id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSSubClass       0\n",
       "MSZoning         0\n",
       "LotFrontage      0\n",
       "LotArea          0\n",
       "Street           0\n",
       "                ..\n",
       "MoSold           0\n",
       "YrSold           0\n",
       "SaleType         0\n",
       "SaleCondition    0\n",
       "SalePrice        0\n",
       "Length: 75, dtype: int64"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x133eaeeb8>"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 75)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>...</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Street LotShape LandContour  \\\n",
       "0          60       RL         65.0     8450   Pave      Reg         Lvl   \n",
       "1          20       RL         80.0     9600   Pave      Reg         Lvl   \n",
       "2          60       RL         68.0    11250   Pave      IR1         Lvl   \n",
       "3          70       RL         60.0     9550   Pave      IR1         Lvl   \n",
       "4          60       RL         84.0    14260   Pave      IR1         Lvl   \n",
       "\n",
       "  Utilities LotConfig LandSlope  ... EnclosedPorch 3SsnPorch ScreenPorch  \\\n",
       "0    AllPub    Inside       Gtl  ...             0         0           0   \n",
       "1    AllPub       FR2       Gtl  ...             0         0           0   \n",
       "2    AllPub    Inside       Gtl  ...             0         0           0   \n",
       "3    AllPub    Corner       Gtl  ...           272         0           0   \n",
       "4    AllPub       FR2       Gtl  ...             0         0           0   \n",
       "\n",
       "  PoolArea MiscVal  MoSold  YrSold  SaleType  SaleCondition SalePrice  \n",
       "0        0       0       2    2008        WD         Normal    208500  \n",
       "1        0       0       5    2007        WD         Normal    181500  \n",
       "2        0       0       9    2008        WD         Normal    223500  \n",
       "3        0       0       2    2006        WD        Abnorml    140000  \n",
       "4        0       0      12    2008        WD         Normal    250000  \n",
       "\n",
       "[5 rows x 75 columns]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data mining(fillna, drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>20</td>\n",
       "      <td>RH</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gar2</td>\n",
       "      <td>12500</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9978</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>120</td>\n",
       "      <td>RL</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5005</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>HLS</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0  1461          20       RH         80.0    11622   Pave   NaN      Reg   \n",
       "1  1462          20       RL         81.0    14267   Pave   NaN      IR1   \n",
       "2  1463          60       RL         74.0    13830   Pave   NaN      IR1   \n",
       "3  1464          60       RL         78.0     9978   Pave   NaN      IR1   \n",
       "4  1465         120       RL         43.0     5005   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... ScreenPorch PoolArea PoolQC  Fence MiscFeature  \\\n",
       "0         Lvl    AllPub  ...         120        0    NaN  MnPrv         NaN   \n",
       "1         Lvl    AllPub  ...           0        0    NaN    NaN        Gar2   \n",
       "2         Lvl    AllPub  ...           0        0    NaN  MnPrv         NaN   \n",
       "3         Lvl    AllPub  ...           0        0    NaN    NaN         NaN   \n",
       "4         HLS    AllPub  ...         144        0    NaN    NaN         NaN   \n",
       "\n",
       "  MiscVal MoSold  YrSold  SaleType  SaleCondition  \n",
       "0       0      6    2010        WD         Normal  \n",
       "1   12500      6    2010        WD         Normal  \n",
       "2       0      3    2010        WD         Normal  \n",
       "3       0      6    2010        WD         Normal  \n",
       "4       0      1    2010        WD         Normal  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 80)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1459 entries, 0 to 1458\n",
      "Data columns (total 80 columns):\n",
      "Id               1459 non-null int64\n",
      "MSSubClass       1459 non-null int64\n",
      "MSZoning         1455 non-null object\n",
      "LotFrontage      1232 non-null float64\n",
      "LotArea          1459 non-null int64\n",
      "Street           1459 non-null object\n",
      "Alley            107 non-null object\n",
      "LotShape         1459 non-null object\n",
      "LandContour      1459 non-null object\n",
      "Utilities        1457 non-null object\n",
      "LotConfig        1459 non-null object\n",
      "LandSlope        1459 non-null object\n",
      "Neighborhood     1459 non-null object\n",
      "Condition1       1459 non-null object\n",
      "Condition2       1459 non-null object\n",
      "BldgType         1459 non-null object\n",
      "HouseStyle       1459 non-null object\n",
      "OverallQual      1459 non-null int64\n",
      "OverallCond      1459 non-null int64\n",
      "YearBuilt        1459 non-null int64\n",
      "YearRemodAdd     1459 non-null int64\n",
      "RoofStyle        1459 non-null object\n",
      "RoofMatl         1459 non-null object\n",
      "Exterior1st      1458 non-null object\n",
      "Exterior2nd      1458 non-null object\n",
      "MasVnrType       1443 non-null object\n",
      "MasVnrArea       1444 non-null float64\n",
      "ExterQual        1459 non-null object\n",
      "ExterCond        1459 non-null object\n",
      "Foundation       1459 non-null object\n",
      "BsmtQual         1415 non-null object\n",
      "BsmtCond         1414 non-null object\n",
      "BsmtExposure     1415 non-null object\n",
      "BsmtFinType1     1417 non-null object\n",
      "BsmtFinSF1       1458 non-null float64\n",
      "BsmtFinType2     1417 non-null object\n",
      "BsmtFinSF2       1458 non-null float64\n",
      "BsmtUnfSF        1458 non-null float64\n",
      "TotalBsmtSF      1458 non-null float64\n",
      "Heating          1459 non-null object\n",
      "HeatingQC        1459 non-null object\n",
      "CentralAir       1459 non-null object\n",
      "Electrical       1459 non-null object\n",
      "1stFlrSF         1459 non-null int64\n",
      "2ndFlrSF         1459 non-null int64\n",
      "LowQualFinSF     1459 non-null int64\n",
      "GrLivArea        1459 non-null int64\n",
      "BsmtFullBath     1457 non-null float64\n",
      "BsmtHalfBath     1457 non-null float64\n",
      "FullBath         1459 non-null int64\n",
      "HalfBath         1459 non-null int64\n",
      "BedroomAbvGr     1459 non-null int64\n",
      "KitchenAbvGr     1459 non-null int64\n",
      "KitchenQual      1458 non-null object\n",
      "TotRmsAbvGrd     1459 non-null int64\n",
      "Functional       1457 non-null object\n",
      "Fireplaces       1459 non-null int64\n",
      "FireplaceQu      729 non-null object\n",
      "GarageType       1383 non-null object\n",
      "GarageYrBlt      1381 non-null float64\n",
      "GarageFinish     1381 non-null object\n",
      "GarageCars       1458 non-null float64\n",
      "GarageArea       1458 non-null float64\n",
      "GarageQual       1381 non-null object\n",
      "GarageCond       1381 non-null object\n",
      "PavedDrive       1459 non-null object\n",
      "WoodDeckSF       1459 non-null int64\n",
      "OpenPorchSF      1459 non-null int64\n",
      "EnclosedPorch    1459 non-null int64\n",
      "3SsnPorch        1459 non-null int64\n",
      "ScreenPorch      1459 non-null int64\n",
      "PoolArea         1459 non-null int64\n",
      "PoolQC           3 non-null object\n",
      "Fence            290 non-null object\n",
      "MiscFeature      51 non-null object\n",
      "MiscVal          1459 non-null int64\n",
      "MoSold           1459 non-null int64\n",
      "YrSold           1459 non-null int64\n",
      "SaleType         1458 non-null object\n",
      "SaleCondition    1459 non-null object\n",
      "dtypes: float64(11), int64(26), object(43)\n",
      "memory usage: 912.0+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['LotFrontage'] = test['LotFrontage'].fillna(test['LotFrontage'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['MSZoning'] = test['MSZoning'].fillna(test['MSZoning'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Utilities'] = test['Utilities'].fillna(test['Utilities'].mode()[0])\n",
    "test['Exterior1st'] = test['Exterior1st'].fillna(test['Exterior1st'].mode()[0])\n",
    "test['Exterior2nd'] = test['Exterior2nd'].fillna(test['Exterior2nd'].mode()[0])\n",
    "test['MasVnrType'] = test['MasVnrType'].fillna(test['MasVnrType'].mode()[0])\n",
    "test['MasVnrArea'] = test['MasVnrArea'].fillna(test['MasVnrArea'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['BsmtQual'] = test['BsmtQual'].fillna(test['BsmtQual'].mode()[0])\n",
    "test['BsmtCond'] = test['BsmtCond'].fillna(test['BsmtCond'].mode()[0])\n",
    "test['BsmtExposure'] = test['BsmtExposure'].fillna(test['BsmtExposure'].mode()[0])\n",
    "test['BsmtFinType1'] = test['BsmtFinType1'].fillna(test['BsmtFinType1'].mode()[0])\n",
    "test['BsmtFinType2'] = test['BsmtFinType2'].fillna(test['BsmtFinType2'].mode()[0])\n",
    "test['BsmtFinSF1'] = test['BsmtFinSF1'].fillna(test['BsmtFinSF1'].mean())\n",
    "test['BsmtFinSF2'] = test['BsmtFinSF2'].fillna(test['BsmtFinSF2'].mean())\n",
    "test['BsmtUnfSF'] = test['BsmtUnfSF'].fillna(test['BsmtUnfSF'].mean())\n",
    "test['TotalBsmtSF'] = test['TotalBsmtSF'].fillna(test['TotalBsmtSF'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['BsmtFullBath'] = test['BsmtFullBath'].fillna(test['BsmtFullBath'].mode()[0])\n",
    "test['BsmtHalfBath'] = test['BsmtHalfBath'].fillna(test['BsmtHalfBath'].mode()[0])\n",
    "test['KitchenQual'] = test['KitchenQual'].fillna(test['KitchenQual'].mode()[0])\n",
    "test['Functional'] = test['Functional'].fillna(test['Functional'].mode()[0])\n",
    "test['FireplaceQu'] = test['FireplaceQu'].fillna(test['FireplaceQu'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['GarageType'] = test['GarageType'].fillna(test['GarageType'].mode()[0])\n",
    "test['GarageFinish'] = test['GarageFinish'].fillna(test['GarageFinish'].mode()[0])\n",
    "test['GarageCars'] = test['GarageCars'].fillna(test['GarageCars'].mean())\n",
    "test['GarageArea'] = test['GarageArea'].fillna(test['GarageArea'].mean())\n",
    "test['GarageQual'] = test['GarageQual'].fillna(test['GarageQual'].mode()[0])\n",
    "test['GarageCond'] = test['GarageCond'].fillna(test['GarageCond'].mode()[0])\n",
    "test['SaleType'] = test['SaleType'].fillna(test['SaleType'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop(['Alley', 'PoolQC', 'Fence', 'MiscFeature', 'GarageYrBlt'],axis=1,inplace=True)\n",
    "test.drop(['Id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSSubClass       0\n",
       "MSZoning         0\n",
       "LotFrontage      0\n",
       "LotArea          0\n",
       "Street           0\n",
       "                ..\n",
       "MiscVal          0\n",
       "MoSold           0\n",
       "YrSold           0\n",
       "SaleType         0\n",
       "SaleCondition    0\n",
       "Length: 74, dtype: int64"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x133d854a8>"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(test.isnull(),yticklabels=False,cbar=False,cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 74)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>...</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>RH</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12500</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9978</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120</td>\n",
       "      <td>RL</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5005</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>HLS</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Street LotShape LandContour  \\\n",
       "0          20       RH         80.0    11622   Pave      Reg         Lvl   \n",
       "1          20       RL         81.0    14267   Pave      IR1         Lvl   \n",
       "2          60       RL         74.0    13830   Pave      IR1         Lvl   \n",
       "3          60       RL         78.0     9978   Pave      IR1         Lvl   \n",
       "4         120       RL         43.0     5005   Pave      IR1         HLS   \n",
       "\n",
       "  Utilities LotConfig LandSlope  ... OpenPorchSF EnclosedPorch 3SsnPorch  \\\n",
       "0    AllPub    Inside       Gtl  ...           0             0         0   \n",
       "1    AllPub    Corner       Gtl  ...          36             0         0   \n",
       "2    AllPub    Inside       Gtl  ...          34             0         0   \n",
       "3    AllPub    Inside       Gtl  ...          36             0         0   \n",
       "4    AllPub    Inside       Gtl  ...          82             0         0   \n",
       "\n",
       "  ScreenPorch PoolArea  MiscVal  MoSold  YrSold  SaleType SaleCondition  \n",
       "0         120        0        0       6    2010        WD        Normal  \n",
       "1           0        0    12500       6    2010        WD        Normal  \n",
       "2           0        0        0       3    2010        WD        Normal  \n",
       "3           0        0        0       6    2010        WD        Normal  \n",
       "4         144        0        0       1    2010        WD        Normal  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['MSZoning','Street','LotShape','LandContour','Utilities','LotConfig','LandSlope','Neighborhood',\n",
    "         'Condition2','BldgType','Condition1','HouseStyle','SaleType',\n",
    "        'SaleCondition','ExterCond',\n",
    "         'ExterQual','Foundation','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n",
    "        'RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','Heating','HeatingQC',\n",
    "         'CentralAir',\n",
    "         'Electrical','KitchenQual','Functional',\n",
    "         'FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','PavedDrive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_onehot_multcols(multcolumns):\n",
    "    df_final=final_df\n",
    "    i=0\n",
    "    for fields in multcolumns:\n",
    "        \n",
    "        print(fields)\n",
    "        df1=pd.get_dummies(final_df[fields],drop_first=True)\n",
    "        \n",
    "        final_df.drop([fields],axis=1,inplace=True)\n",
    "        if i==0:\n",
    "            df_final=df1.copy()\n",
    "        else:\n",
    "            \n",
    "            df_final=pd.concat([df_final,df1],axis=1)\n",
    "        i=i+1\n",
    "       \n",
    "        \n",
    "    df_final=pd.concat([final_df,df_final],axis=1)\n",
    "        \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parkchanjin/anaconda3/envs/ml/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "final_df=pd.concat([train,test],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       208500.0\n",
       "1       181500.0\n",
       "2       223500.0\n",
       "3       140000.0\n",
       "4       250000.0\n",
       "          ...   \n",
       "1454         NaN\n",
       "1455         NaN\n",
       "1456         NaN\n",
       "1457         NaN\n",
       "1458         NaN\n",
       "Name: SalePrice, Length: 2919, dtype: float64"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 75)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSZoning\n",
      "Street\n",
      "LotShape\n",
      "LandContour\n",
      "Utilities\n",
      "LotConfig\n",
      "LandSlope\n",
      "Neighborhood\n",
      "Condition2\n",
      "BldgType\n",
      "Condition1\n",
      "HouseStyle\n",
      "SaleType\n",
      "SaleCondition\n",
      "ExterCond\n",
      "ExterQual\n",
      "Foundation\n",
      "BsmtQual\n",
      "BsmtCond\n",
      "BsmtExposure\n",
      "BsmtFinType1\n",
      "BsmtFinType2\n",
      "RoofStyle\n",
      "RoofMatl\n",
      "Exterior1st\n",
      "Exterior2nd\n",
      "MasVnrType\n",
      "Heating\n",
      "HeatingQC\n",
      "CentralAir\n",
      "Electrical\n",
      "KitchenQual\n",
      "Functional\n",
      "FireplaceQu\n",
      "GarageType\n",
      "GarageFinish\n",
      "GarageQual\n",
      "GarageCond\n",
      "PavedDrive\n"
     ]
    }
   ],
   "source": [
    "final_df=category_onehot_multcols(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 236)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.loc[:,~final_df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 176)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>...</th>\n",
       "      <th>Min1</th>\n",
       "      <th>Min2</th>\n",
       "      <th>Typ</th>\n",
       "      <th>Attchd</th>\n",
       "      <th>Basment</th>\n",
       "      <th>BuiltIn</th>\n",
       "      <th>CarPort</th>\n",
       "      <th>Detchd</th>\n",
       "      <th>RFn</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>486.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>272</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>655.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>490.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 176 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1stFlrSF  2ndFlrSF  3SsnPorch  BedroomAbvGr  BsmtFinSF1  BsmtFinSF2  \\\n",
       "0       856       854          0             3       706.0         0.0   \n",
       "1      1262         0          0             3       978.0         0.0   \n",
       "2       920       866          0             3       486.0         0.0   \n",
       "3       961       756          0             3       216.0         0.0   \n",
       "4      1145      1053          0             4       655.0         0.0   \n",
       "\n",
       "   BsmtFullBath  BsmtHalfBath  BsmtUnfSF  EnclosedPorch  ...  Min1  Min2  Typ  \\\n",
       "0           1.0           0.0      150.0              0  ...     0     0    1   \n",
       "1           0.0           1.0      284.0              0  ...     0     0    1   \n",
       "2           1.0           0.0      434.0              0  ...     0     0    1   \n",
       "3           1.0           0.0      540.0            272  ...     0     0    1   \n",
       "4           1.0           0.0      490.0              0  ...     0     0    1   \n",
       "\n",
       "   Attchd  Basment  BuiltIn  CarPort  Detchd  RFn  P  \n",
       "0       1        0        0        0       0    1  0  \n",
       "1       1        0        0        0       0    1  0  \n",
       "2       1        0        0        0       0    1  0  \n",
       "3       0        0        0        0       1    0  0  \n",
       "4       1        0        0        0       0    1  0  \n",
       "\n",
       "[5 rows x 176 columns]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = final_df.iloc[:1460,:]\n",
    "test_data = final_df.iloc[1460:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>...</th>\n",
       "      <th>Min1</th>\n",
       "      <th>Min2</th>\n",
       "      <th>Typ</th>\n",
       "      <th>Attchd</th>\n",
       "      <th>Basment</th>\n",
       "      <th>BuiltIn</th>\n",
       "      <th>CarPort</th>\n",
       "      <th>Detchd</th>\n",
       "      <th>RFn</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>486.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>272</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>655.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>490.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 176 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1stFlrSF  2ndFlrSF  3SsnPorch  BedroomAbvGr  BsmtFinSF1  BsmtFinSF2  \\\n",
       "0       856       854          0             3       706.0         0.0   \n",
       "1      1262         0          0             3       978.0         0.0   \n",
       "2       920       866          0             3       486.0         0.0   \n",
       "3       961       756          0             3       216.0         0.0   \n",
       "4      1145      1053          0             4       655.0         0.0   \n",
       "\n",
       "   BsmtFullBath  BsmtHalfBath  BsmtUnfSF  EnclosedPorch  ...  Min1  Min2  Typ  \\\n",
       "0           1.0           0.0      150.0              0  ...     0     0    1   \n",
       "1           0.0           1.0      284.0              0  ...     0     0    1   \n",
       "2           1.0           0.0      434.0              0  ...     0     0    1   \n",
       "3           1.0           0.0      540.0            272  ...     0     0    1   \n",
       "4           1.0           0.0      490.0              0  ...     0     0    1   \n",
       "\n",
       "   Attchd  Basment  BuiltIn  CarPort  Detchd  RFn  P  \n",
       "0       1        0        0        0       0    1  0  \n",
       "1       1        0        0        0       0    1  0  \n",
       "2       1        0        0        0       0    1  0  \n",
       "3       0        0        0        0       1    0  0  \n",
       "4       1        0        0        0       0    1  0  \n",
       "\n",
       "[5 rows x 176 columns]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>...</th>\n",
       "      <th>Min1</th>\n",
       "      <th>Min2</th>\n",
       "      <th>Typ</th>\n",
       "      <th>Attchd</th>\n",
       "      <th>Basment</th>\n",
       "      <th>BuiltIn</th>\n",
       "      <th>CarPort</th>\n",
       "      <th>Detchd</th>\n",
       "      <th>RFn</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>468.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1329</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>923.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>406.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>928</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>791.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>926</td>\n",
       "      <td>678</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>602.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>324.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1280</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1017.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 176 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1stFlrSF  2ndFlrSF  3SsnPorch  BedroomAbvGr  BsmtFinSF1  BsmtFinSF2  \\\n",
       "0       896         0          0             2       468.0       144.0   \n",
       "1      1329         0          0             3       923.0         0.0   \n",
       "2       928       701          0             3       791.0         0.0   \n",
       "3       926       678          0             3       602.0         0.0   \n",
       "4      1280         0          0             2       263.0         0.0   \n",
       "\n",
       "   BsmtFullBath  BsmtHalfBath  BsmtUnfSF  EnclosedPorch  ...  Min1  Min2  Typ  \\\n",
       "0           0.0           0.0      270.0              0  ...     0     0    1   \n",
       "1           0.0           0.0      406.0              0  ...     0     0    1   \n",
       "2           0.0           0.0      137.0              0  ...     0     0    1   \n",
       "3           0.0           0.0      324.0              0  ...     0     0    1   \n",
       "4           0.0           0.0     1017.0              0  ...     0     0    1   \n",
       "\n",
       "   Attchd  Basment  BuiltIn  CarPort  Detchd  RFn  P  \n",
       "0       1        0        0        0       0    0  0  \n",
       "1       1        0        0        0       0    0  0  \n",
       "2       1        0        0        0       0    0  0  \n",
       "3       1        0        0        0       0    0  0  \n",
       "4       1        0        0        0       0    1  0  \n",
       "\n",
       "[5 rows x 176 columns]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 176)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.drop(['SalePrice'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 175)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop(['SalePrice'],axis=1)\n",
    "y_train = train_data['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parkchanjin/anaconda3/envs/ml/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/Users/parkchanjin/anaconda3/envs/ml/lib/python3.7/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bytree=1, gamma=0, importance_type='gain',\n",
       "             learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "             min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "             nthread=None, objective='reg:linear', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "             subsample=1)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor=xgboost.XGBRegressor()\n",
    "classifier=xgboost.XGBRegressor()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([123579.086, 153624.25 , 179177.39 , ..., 165475.47 , 125263.01 ,\n",
       "       240619.03 ], dtype=float32)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=pd.DataFrame(y_pred)\n",
    "sub_df=pd.read_csv('datasets/housing/sample_submission.csv')\n",
    "datasets=pd.concat([sub_df['Id'],pred],axis=1)\n",
    "datasets.columns=['Id','SalePrice']\n",
    "datasets.to_csv('clfResultCJ.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster=['gbtree','gblinear']\n",
    "base_score=[0.25,0.5,0.75,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [100, 500, 900, 1100, 1500]\n",
    "max_depth = [2, 3, 5, 10, 15]\n",
    "booster=['gbtree','gblinear']\n",
    "learning_rate=[0.05,0.1,0.15,0.20]\n",
    "min_child_weight=[1,2,3,4]\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "hyperparameter_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_depth':max_depth,\n",
    "    'learning_rate':learning_rate,\n",
    "    'min_child_weight':min_child_weight,\n",
    "    'booster':booster,\n",
    "    'base_score':base_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "random_cv = RandomizedSearchCV(estimator=regressor,\n",
    "            param_distributions=hyperparameter_grid,\n",
    "            cv=5, n_iter=50,\n",
    "            scoring = 'neg_mean_absolute_error',n_jobs = 4,\n",
    "            verbose = 5, \n",
    "            return_train_score = True,\n",
    "            random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:   41.6s\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed: 11.0min\n",
      "[Parallel(n_jobs=4)]: Done 250 out of 250 | elapsed: 17.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "                   estimator=XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                                          colsample_bylevel=1,\n",
       "                                          colsample_bytree=1, gamma=0,\n",
       "                                          importance_type='gain',\n",
       "                                          learning_rate=0.1, max_delta_step=0,\n",
       "                                          max_depth=3, min_child_weight=1,\n",
       "                                          missing=None, n_estimators=100,\n",
       "                                          n_jobs=1, nthread=None,\n",
       "                                          objective='reg:linear',\n",
       "                                          random_state=0, reg_alpha=0,\n",
       "                                          r...\n",
       "                   iid='warn', n_iter=50, n_jobs=4,\n",
       "                   param_distributions={'base_score': [0.25, 0.5, 0.75, 1],\n",
       "                                        'booster': ['gbtree', 'gblinear'],\n",
       "                                        'learning_rate': [0.05, 0.1, 0.15, 0.2],\n",
       "                                        'max_depth': [2, 3, 5, 10, 15],\n",
       "                                        'min_child_weight': [1, 2, 3, 4],\n",
       "                                        'n_estimators': [100, 500, 900, 1100,\n",
       "                                                         1500]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=True, scoring='neg_mean_absolute_error',\n",
       "                   verbose=5)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bytree=1, gamma=0, importance_type='gain',\n",
       "             learning_rate=0.1, max_delta_step=0, max_depth=2,\n",
       "             min_child_weight=1, missing=None, n_estimators=900, n_jobs=1,\n",
       "             nthread=None, objective='reg:linear', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "             subsample=1)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor=xgboost.XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "       max_depth=2, min_child_weight=1, missing=None, n_estimators=900,\n",
    "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       silent=True, subsample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bytree=1, gamma=0, importance_type='gain',\n",
       "             learning_rate=0.1, max_delta_step=0, max_depth=2,\n",
       "             min_child_weight=1, missing=None, n_estimators=900, n_jobs=1,\n",
       "             nthread=None, objective='reg:linear', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "             subsample=1)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'finalized_model.pkl'\n",
    "pickle.dump(classifier, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([122548.37, 160994.97, 184981.12, ..., 175130.5 , 120274.3 ,\n",
       "       237327.53], dtype=float32)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=pd.DataFrame(y_pred)\n",
    "sub_df=pd.read_csv('datasets/housing/sample_submission.csv')\n",
    "datasets=pd.concat([sub_df['Id'],pred],axis=1)\n",
    "datasets.columns=['Id','SalePrice']\n",
    "datasets.to_csv('regResultCJ.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. using deeplearning (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parkchanjin/anaconda3/envs/ml/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=175, units=50, kernel_initializer=\"he_uniform\")`\n",
      "  if sys.path[0] == '':\n",
      "/Users/parkchanjin/anaconda3/envs/ml/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=25, kernel_initializer=\"he_uniform\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/parkchanjin/anaconda3/envs/ml/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=50, kernel_initializer=\"he_uniform\")`\n",
      "/Users/parkchanjin/anaconda3/envs/ml/lib/python3.7/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1, kernel_initializer=\"he_uniform\")`\n",
      "/Users/parkchanjin/anaconda3/envs/ml/lib/python3.7/site-packages/ipykernel_launcher.py:26: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1000\n",
      "1168/1168 [==============================] - 1s 467us/step - loss: 158189.5480 - val_loss: 82162.9217\n",
      "Epoch 2/1000\n",
      "1168/1168 [==============================] - 0s 194us/step - loss: 67067.5005 - val_loss: 62938.6809\n",
      "Epoch 3/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 62570.8210 - val_loss: 60760.9275\n",
      "Epoch 4/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 59285.8928 - val_loss: 58722.7482\n",
      "Epoch 5/1000\n",
      "1168/1168 [==============================] - 0s 139us/step - loss: 55185.7171 - val_loss: 56989.5469\n",
      "Epoch 6/1000\n",
      "1168/1168 [==============================] - 0s 152us/step - loss: 53776.0585 - val_loss: 55536.7825\n",
      "Epoch 7/1000\n",
      "1168/1168 [==============================] - 0s 132us/step - loss: 51164.6637 - val_loss: 53762.3680\n",
      "Epoch 8/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 48274.2163 - val_loss: 51907.8119\n",
      "Epoch 9/1000\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 46063.6212 - val_loss: 50326.7897\n",
      "Epoch 10/1000\n",
      "1168/1168 [==============================] - 0s 157us/step - loss: 43320.5049 - val_loss: 49394.6168\n",
      "Epoch 11/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 41076.0057 - val_loss: 48134.3924\n",
      "Epoch 12/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 39752.5165 - val_loss: 46687.1599\n",
      "Epoch 13/1000\n",
      "1168/1168 [==============================] - 0s 158us/step - loss: 38257.5172 - val_loss: 46719.3388\n",
      "Epoch 14/1000\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 37553.9954 - val_loss: 45775.3062\n",
      "Epoch 15/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 37806.7808 - val_loss: 46058.3727\n",
      "Epoch 16/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 36539.3159 - val_loss: 45807.1628\n",
      "Epoch 17/1000\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 36235.8727 - val_loss: 45885.4731\n",
      "Epoch 18/1000\n",
      "1168/1168 [==============================] - 0s 136us/step - loss: 36025.1584 - val_loss: 45657.1704\n",
      "Epoch 19/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 35608.7691 - val_loss: 45558.6553\n",
      "Epoch 20/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 35922.9346 - val_loss: 45521.5921\n",
      "Epoch 21/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 36138.6361 - val_loss: 45336.0056\n",
      "Epoch 22/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 35715.5851 - val_loss: 45320.8931\n",
      "Epoch 23/1000\n",
      "1168/1168 [==============================] - 0s 140us/step - loss: 35853.6888 - val_loss: 45977.5173\n",
      "Epoch 24/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 35924.6412 - val_loss: 45467.8887\n",
      "Epoch 25/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 35590.2298 - val_loss: 46952.7861\n",
      "Epoch 26/1000\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 35814.1508 - val_loss: 45412.3549\n",
      "Epoch 27/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 35662.7661 - val_loss: 45127.9782\n",
      "Epoch 28/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 35399.1090 - val_loss: 45273.3219\n",
      "Epoch 29/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 35645.7967 - val_loss: 45056.8678\n",
      "Epoch 30/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 35303.6231 - val_loss: 45726.4505\n",
      "Epoch 31/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 35224.3840 - val_loss: 45028.5988\n",
      "Epoch 32/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 35355.4015 - val_loss: 46640.9572\n",
      "Epoch 33/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 35160.8858 - val_loss: 46112.6104\n",
      "Epoch 34/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 35491.7779 - val_loss: 44969.2178\n",
      "Epoch 35/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 35129.3657 - val_loss: 45924.4108\n",
      "Epoch 36/1000\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 34784.1066 - val_loss: 44917.6581\n",
      "Epoch 37/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 35104.0000 - val_loss: 44944.1208\n",
      "Epoch 38/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 34691.7145 - val_loss: 44802.8406\n",
      "Epoch 39/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 35022.7774 - val_loss: 44780.5643\n",
      "Epoch 40/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 34759.2751 - val_loss: 44726.0260\n",
      "Epoch 41/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 35209.8725 - val_loss: 44901.7416\n",
      "Epoch 42/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 34297.3063 - val_loss: 44781.4103\n",
      "Epoch 43/1000\n",
      "1168/1168 [==============================] - 0s 132us/step - loss: 34966.7994 - val_loss: 44521.4429\n",
      "Epoch 44/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 34515.6119 - val_loss: 46663.9328\n",
      "Epoch 45/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 34457.1484 - val_loss: 44511.9503\n",
      "Epoch 46/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 34906.2819 - val_loss: 44512.0439\n",
      "Epoch 47/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 34500.1799 - val_loss: 44926.1895\n",
      "Epoch 48/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 33850.3276 - val_loss: 44752.2067\n",
      "Epoch 49/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 33790.0147 - val_loss: 45735.9314\n",
      "Epoch 50/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 34303.3978 - val_loss: 45116.1474\n",
      "Epoch 51/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 33723.3099 - val_loss: 45730.3306\n",
      "Epoch 52/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 34227.5744 - val_loss: 45262.5924\n",
      "Epoch 53/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 33726.9620 - val_loss: 44283.1480\n",
      "Epoch 54/1000\n",
      "1168/1168 [==============================] - 0s 108us/step - loss: 34096.9723 - val_loss: 44250.5727\n",
      "Epoch 55/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 33986.2566 - val_loss: 44286.6598\n",
      "Epoch 56/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 33889.0939 - val_loss: 44627.1694\n",
      "Epoch 57/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 33635.8410 - val_loss: 46382.1643\n",
      "Epoch 58/1000\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 34136.3126 - val_loss: 44938.1073\n",
      "Epoch 59/1000\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 33806.3554 - val_loss: 45271.2657\n",
      "Epoch 60/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 33521.2441 - val_loss: 44184.3583\n",
      "Epoch 61/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 33504.2715 - val_loss: 44456.8474\n",
      "Epoch 62/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 33417.6243 - val_loss: 44364.3187\n",
      "Epoch 63/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 33405.6355 - val_loss: 45241.7460\n",
      "Epoch 64/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 33176.2769 - val_loss: 44703.9926\n",
      "Epoch 65/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 32740.1156 - val_loss: 44176.6870\n",
      "Epoch 66/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 33124.5596 - val_loss: 44500.2999\n",
      "Epoch 67/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 32752.2719 - val_loss: 44206.7854\n",
      "Epoch 68/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 33515.9425 - val_loss: 44222.0597\n",
      "Epoch 69/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 33127.5972 - val_loss: 44240.5676\n",
      "Epoch 70/1000\n",
      "1168/1168 [==============================] - 0s 151us/step - loss: 33071.0586 - val_loss: 44384.5326\n",
      "Epoch 71/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 32779.3320 - val_loss: 44444.2961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 32786.4470 - val_loss: 45406.2395\n",
      "Epoch 73/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 32579.8858 - val_loss: 44116.8518\n",
      "Epoch 74/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 32355.8422 - val_loss: 45048.6599\n",
      "Epoch 75/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 32825.3340 - val_loss: 44865.5219\n",
      "Epoch 76/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 32693.6659 - val_loss: 44102.3726\n",
      "Epoch 77/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 32673.0941 - val_loss: 44558.6458\n",
      "Epoch 78/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 32668.9997 - val_loss: 44248.7026\n",
      "Epoch 79/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 32707.0542 - val_loss: 44301.9596\n",
      "Epoch 80/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 32112.1417 - val_loss: 44228.7104\n",
      "Epoch 81/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 32778.1416 - val_loss: 45702.6205\n",
      "Epoch 82/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 32649.2452 - val_loss: 44596.3225\n",
      "Epoch 83/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 32238.3682 - val_loss: 45024.8156\n",
      "Epoch 84/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 32446.4140 - val_loss: 44740.5670\n",
      "Epoch 85/1000\n",
      "1168/1168 [==============================] - 0s 108us/step - loss: 32056.6672 - val_loss: 44325.3738\n",
      "Epoch 86/1000\n",
      "1168/1168 [==============================] - 0s 108us/step - loss: 31970.7526 - val_loss: 44730.4796\n",
      "Epoch 87/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 32218.5587 - val_loss: 44431.8042\n",
      "Epoch 88/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 32080.9535 - val_loss: 46688.1673\n",
      "Epoch 89/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 31869.3497 - val_loss: 45757.0067\n",
      "Epoch 90/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 31954.8828 - val_loss: 44799.9530\n",
      "Epoch 91/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 31816.1668 - val_loss: 44317.6478\n",
      "Epoch 92/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 31680.9503 - val_loss: 44960.1095\n",
      "Epoch 93/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 31473.6624 - val_loss: 44913.5246\n",
      "Epoch 94/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 31990.2634 - val_loss: 45896.5616\n",
      "Epoch 95/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 31751.3495 - val_loss: 44949.7495\n",
      "Epoch 96/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 31759.9378 - val_loss: 45249.3908\n",
      "Epoch 97/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 31785.1878 - val_loss: 44596.8366\n",
      "Epoch 98/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 31440.8124 - val_loss: 44664.3166\n",
      "Epoch 99/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 31573.0245 - val_loss: 45574.3076\n",
      "Epoch 100/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 31224.9971 - val_loss: 44504.5452\n",
      "Epoch 101/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 31250.0475 - val_loss: 44877.4884\n",
      "Epoch 102/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 31696.6150 - val_loss: 44535.4430\n",
      "Epoch 103/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 31452.6266 - val_loss: 44655.4220\n",
      "Epoch 104/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 31462.7185 - val_loss: 45299.3871\n",
      "Epoch 105/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 31190.6329 - val_loss: 44656.3026\n",
      "Epoch 106/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 31203.9197 - val_loss: 44576.5453\n",
      "Epoch 107/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 31247.7283 - val_loss: 44575.1187\n",
      "Epoch 108/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 31543.8147 - val_loss: 44538.0651\n",
      "Epoch 109/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 30872.0032 - val_loss: 45549.2610\n",
      "Epoch 110/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 31290.9871 - val_loss: 44487.6837\n",
      "Epoch 111/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 30998.6105 - val_loss: 45461.5737\n",
      "Epoch 112/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 31614.9237 - val_loss: 44794.8264\n",
      "Epoch 113/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 31159.5842 - val_loss: 45193.6599\n",
      "Epoch 114/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 31449.2937 - val_loss: 45547.2948\n",
      "Epoch 115/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 30985.0724 - val_loss: 44385.1022\n",
      "Epoch 116/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 31138.2756 - val_loss: 45040.2238\n",
      "Epoch 117/1000\n",
      "1168/1168 [==============================] - 0s 204us/step - loss: 31071.0685 - val_loss: 45094.4359\n",
      "Epoch 118/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 30906.6590 - val_loss: 44419.0448\n",
      "Epoch 119/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 31095.4419 - val_loss: 47502.6393\n",
      "Epoch 120/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 30723.7775 - val_loss: 44462.8195\n",
      "Epoch 121/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 31392.3373 - val_loss: 44502.2392\n",
      "Epoch 122/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 30872.6213 - val_loss: 44611.0455\n",
      "Epoch 123/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 30428.8855 - val_loss: 44727.7290\n",
      "Epoch 124/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 31045.7646 - val_loss: 45003.8166\n",
      "Epoch 125/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 30997.6221 - val_loss: 44880.6601\n",
      "Epoch 126/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 30750.6204 - val_loss: 44465.1733\n",
      "Epoch 127/1000\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 30879.8084 - val_loss: 44492.5859\n",
      "Epoch 128/1000\n",
      "1168/1168 [==============================] - 0s 138us/step - loss: 31444.8209 - val_loss: 44801.5546\n",
      "Epoch 129/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 30701.4540 - val_loss: 44412.9435\n",
      "Epoch 130/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 30784.8667 - val_loss: 44323.7384\n",
      "Epoch 131/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 30458.4049 - val_loss: 44943.5325\n",
      "Epoch 132/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 30614.9491 - val_loss: 48926.4391\n",
      "Epoch 133/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 30849.1844 - val_loss: 45151.6278\n",
      "Epoch 134/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 30888.2730 - val_loss: 44861.8466\n",
      "Epoch 135/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 30363.3072 - val_loss: 44301.9483\n",
      "Epoch 136/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 30337.9083 - val_loss: 44414.5982\n",
      "Epoch 137/1000\n",
      "1168/1168 [==============================] - 0s 139us/step - loss: 30411.2128 - val_loss: 46199.9086\n",
      "Epoch 138/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 30718.6944 - val_loss: 44117.5948\n",
      "Epoch 139/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 31093.7707 - val_loss: 44454.2289\n",
      "Epoch 140/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 30254.0750 - val_loss: 45126.3093\n",
      "Epoch 141/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 30283.1369 - val_loss: 45007.7594\n",
      "Epoch 142/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 31157.4816 - val_loss: 44411.7313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 30894.1097 - val_loss: 44983.4288\n",
      "Epoch 144/1000\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 30074.2672 - val_loss: 44433.9566\n",
      "Epoch 145/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 30480.2303 - val_loss: 44463.3137\n",
      "Epoch 146/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 30775.5907 - val_loss: 43864.4297\n",
      "Epoch 147/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 29970.3575 - val_loss: 44271.2943\n",
      "Epoch 148/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 30289.9204 - val_loss: 44475.2953\n",
      "Epoch 149/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 30115.6654 - val_loss: 43631.7109\n",
      "Epoch 150/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 30508.9879 - val_loss: 43676.4377\n",
      "Epoch 151/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 30438.0079 - val_loss: 44083.7933\n",
      "Epoch 152/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 29834.9122 - val_loss: 44954.9578\n",
      "Epoch 153/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 29950.9061 - val_loss: 44429.0741\n",
      "Epoch 154/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 29619.7556 - val_loss: 44235.9240\n",
      "Epoch 155/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 29606.9948 - val_loss: 46007.4531\n",
      "Epoch 156/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 29704.4999 - val_loss: 44066.0072\n",
      "Epoch 157/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 29759.7244 - val_loss: 43851.0821\n",
      "Epoch 158/1000\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 30062.8436 - val_loss: 43754.5942\n",
      "Epoch 159/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 29881.1643 - val_loss: 43760.8251\n",
      "Epoch 160/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 29741.8951 - val_loss: 43767.4571\n",
      "Epoch 161/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 30150.5482 - val_loss: 44003.7189\n",
      "Epoch 162/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 29724.1535 - val_loss: 44282.1791\n",
      "Epoch 163/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 29678.5252 - val_loss: 43750.0161\n",
      "Epoch 164/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 29334.1300 - val_loss: 43572.3410\n",
      "Epoch 165/1000\n",
      "1168/1168 [==============================] - 0s 202us/step - loss: 29671.5166 - val_loss: 43500.6003\n",
      "Epoch 166/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 29458.4415 - val_loss: 44073.8837\n",
      "Epoch 167/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 29487.2345 - val_loss: 44630.9128\n",
      "Epoch 168/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 29571.8479 - val_loss: 44087.3943\n",
      "Epoch 169/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 29218.8859 - val_loss: 43641.6387\n",
      "Epoch 170/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 29233.6654 - val_loss: 43683.6232\n",
      "Epoch 171/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 29481.1847 - val_loss: 44075.8241\n",
      "Epoch 172/1000\n",
      "1168/1168 [==============================] - 0s 151us/step - loss: 29488.1678 - val_loss: 44365.3832\n",
      "Epoch 173/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 30034.4032 - val_loss: 46599.9723\n",
      "Epoch 174/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 29542.5729 - val_loss: 44426.5420\n",
      "Epoch 175/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 29292.5730 - val_loss: 44149.7525\n",
      "Epoch 176/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 29446.9706 - val_loss: 43872.3088\n",
      "Epoch 177/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 29436.8520 - val_loss: 44499.3213\n",
      "Epoch 178/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 29195.0340 - val_loss: 43546.9628\n",
      "Epoch 179/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 28859.9959 - val_loss: 43452.4759\n",
      "Epoch 180/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 28891.6410 - val_loss: 44003.3901\n",
      "Epoch 181/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 29146.8169 - val_loss: 43952.0500\n",
      "Epoch 182/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 28733.4656 - val_loss: 44087.3610\n",
      "Epoch 183/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 29407.1520 - val_loss: 44896.7369\n",
      "Epoch 184/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 29518.0812 - val_loss: 43254.5336\n",
      "Epoch 185/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 28839.1429 - val_loss: 43894.0380\n",
      "Epoch 186/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 28879.0539 - val_loss: 43339.8936\n",
      "Epoch 187/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 28917.5200 - val_loss: 43841.9523\n",
      "Epoch 188/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 28591.4729 - val_loss: 43493.9648\n",
      "Epoch 189/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 29099.4199 - val_loss: 43992.3131\n",
      "Epoch 190/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 28589.6648 - val_loss: 44270.5978\n",
      "Epoch 191/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 28820.4567 - val_loss: 43486.0814\n",
      "Epoch 192/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 28757.1678 - val_loss: 43436.4454\n",
      "Epoch 193/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 28493.6372 - val_loss: 43418.2698\n",
      "Epoch 194/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 28617.7800 - val_loss: 44002.3650\n",
      "Epoch 195/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 28528.5713 - val_loss: 44091.0276\n",
      "Epoch 196/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 28168.6342 - val_loss: 43647.7335\n",
      "Epoch 197/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 28714.8199 - val_loss: 42931.3599\n",
      "Epoch 198/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 28272.1401 - val_loss: 44632.2720\n",
      "Epoch 199/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 28335.8714 - val_loss: 43441.9976\n",
      "Epoch 200/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 28300.9261 - val_loss: 43523.2567\n",
      "Epoch 201/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 28016.0688 - val_loss: 43510.2556\n",
      "Epoch 202/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 27754.7128 - val_loss: 42846.7473\n",
      "Epoch 203/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 28821.9120 - val_loss: 43137.9438\n",
      "Epoch 204/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 28082.6962 - val_loss: 44474.0883\n",
      "Epoch 205/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 28406.2227 - val_loss: 43189.7091\n",
      "Epoch 206/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 28389.1804 - val_loss: 43730.3446\n",
      "Epoch 207/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 28193.1909 - val_loss: 43569.1707\n",
      "Epoch 208/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 28166.4462 - val_loss: 42894.4768\n",
      "Epoch 209/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 28009.4662 - val_loss: 43182.3152\n",
      "Epoch 210/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 28277.8618 - val_loss: 43485.0644\n",
      "Epoch 211/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 28292.7458 - val_loss: 42601.5809\n",
      "Epoch 212/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 28367.7030 - val_loss: 44077.4408\n",
      "Epoch 213/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 27927.7423 - val_loss: 43353.4514\n",
      "Epoch 214/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 27901.0991 - val_loss: 43213.8555\n",
      "Epoch 215/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 27983.7987 - val_loss: 43874.6830\n",
      "Epoch 216/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 28174.7577 - val_loss: 43896.1582\n",
      "Epoch 217/1000\n",
      "1168/1168 [==============================] - 0s 201us/step - loss: 28269.5426 - val_loss: 45488.8653\n",
      "Epoch 218/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 27972.6641 - val_loss: 42744.3476\n",
      "Epoch 219/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 27640.3938 - val_loss: 43566.2169\n",
      "Epoch 220/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 27359.8292 - val_loss: 44156.7201\n",
      "Epoch 221/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 27942.6131 - val_loss: 43098.3986\n",
      "Epoch 222/1000\n",
      "1168/1168 [==============================] - 0s 138us/step - loss: 27967.0357 - val_loss: 43323.4013\n",
      "Epoch 223/1000\n",
      "1168/1168 [==============================] - 0s 139us/step - loss: 27598.0565 - val_loss: 43071.6533\n",
      "Epoch 224/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 27361.0433 - val_loss: 42899.2792\n",
      "Epoch 225/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 27157.0030 - val_loss: 43368.4422\n",
      "Epoch 226/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 27176.0522 - val_loss: 43829.6787\n",
      "Epoch 227/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 26945.8472 - val_loss: 42840.1299\n",
      "Epoch 228/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 27623.2149 - val_loss: 42789.0064\n",
      "Epoch 229/1000\n",
      "1168/1168 [==============================] - 0s 160us/step - loss: 27947.5414 - val_loss: 43216.9323\n",
      "Epoch 230/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 27379.3714 - val_loss: 43151.4751\n",
      "Epoch 231/1000\n",
      "1168/1168 [==============================] - 0s 159us/step - loss: 27401.2412 - val_loss: 42367.7967\n",
      "Epoch 232/1000\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 26919.8390 - val_loss: 43232.8392\n",
      "Epoch 233/1000\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 27441.0162 - val_loss: 43421.0859\n",
      "Epoch 234/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 27143.6928 - val_loss: 42860.8012\n",
      "Epoch 235/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 26857.3905 - val_loss: 42889.7414\n",
      "Epoch 236/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 26907.4210 - val_loss: 42637.1747\n",
      "Epoch 237/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 26598.2406 - val_loss: 43868.2863\n",
      "Epoch 238/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 26751.4614 - val_loss: 42774.4489\n",
      "Epoch 239/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 27169.5842 - val_loss: 43065.1378\n",
      "Epoch 240/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 26850.2231 - val_loss: 43192.9642\n",
      "Epoch 241/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 26986.1914 - val_loss: 42538.9052\n",
      "Epoch 242/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 26722.4630 - val_loss: 42860.6307\n",
      "Epoch 243/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 26979.0062 - val_loss: 42596.4754\n",
      "Epoch 244/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 26682.9103 - val_loss: 42276.6833\n",
      "Epoch 245/1000\n",
      "1168/1168 [==============================] - 0s 160us/step - loss: 26894.5531 - val_loss: 42844.2458\n",
      "Epoch 246/1000\n",
      "1168/1168 [==============================] - 0s 152us/step - loss: 26435.1457 - val_loss: 43333.3159\n",
      "Epoch 247/1000\n",
      "1168/1168 [==============================] - 0s 143us/step - loss: 25978.5765 - val_loss: 44808.5635\n",
      "Epoch 248/1000\n",
      "1168/1168 [==============================] - 0s 161us/step - loss: 27191.6315 - val_loss: 42352.0716\n",
      "Epoch 249/1000\n",
      "1168/1168 [==============================] - 0s 163us/step - loss: 26813.0844 - val_loss: 42604.5336\n",
      "Epoch 250/1000\n",
      "1168/1168 [==============================] - 0s 162us/step - loss: 26338.9673 - val_loss: 43158.2996\n",
      "Epoch 251/1000\n",
      "1168/1168 [==============================] - 0s 160us/step - loss: 26502.9783 - val_loss: 42798.6103\n",
      "Epoch 252/1000\n",
      "1168/1168 [==============================] - 0s 151us/step - loss: 25991.2405 - val_loss: 42446.3078\n",
      "Epoch 253/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 26729.3724 - val_loss: 43054.6391\n",
      "Epoch 254/1000\n",
      "1168/1168 [==============================] - 0s 154us/step - loss: 26924.8845 - val_loss: 42208.4572\n",
      "Epoch 255/1000\n",
      "1168/1168 [==============================] - 0s 150us/step - loss: 26198.9637 - val_loss: 42472.5889\n",
      "Epoch 256/1000\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 26287.6863 - val_loss: 42262.8220\n",
      "Epoch 257/1000\n",
      "1168/1168 [==============================] - 0s 155us/step - loss: 26207.4722 - val_loss: 42810.2684\n",
      "Epoch 258/1000\n",
      "1168/1168 [==============================] - 0s 153us/step - loss: 25804.5678 - val_loss: 42847.7623\n",
      "Epoch 259/1000\n",
      "1168/1168 [==============================] - 0s 136us/step - loss: 25924.2706 - val_loss: 42282.5298\n",
      "Epoch 260/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 26223.6335 - val_loss: 41331.0492\n",
      "Epoch 261/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 26285.6915 - val_loss: 42228.6074\n",
      "Epoch 262/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 26441.2947 - val_loss: 42831.5464\n",
      "Epoch 263/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 25816.1570 - val_loss: 41924.5379\n",
      "Epoch 264/1000\n",
      "1168/1168 [==============================] - 0s 140us/step - loss: 25583.4460 - val_loss: 43400.7515\n",
      "Epoch 265/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 26131.1781 - val_loss: 41963.8121\n",
      "Epoch 266/1000\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 25663.6331 - val_loss: 42267.3577\n",
      "Epoch 267/1000\n",
      "1168/1168 [==============================] - 0s 153us/step - loss: 26042.2459 - val_loss: 41875.6973\n",
      "Epoch 268/1000\n",
      "1168/1168 [==============================] - 0s 142us/step - loss: 26080.0952 - val_loss: 41929.8334\n",
      "Epoch 269/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 25559.9702 - val_loss: 42036.9733\n",
      "Epoch 270/1000\n",
      "1168/1168 [==============================] - 0s 155us/step - loss: 25932.1480 - val_loss: 43740.1030\n",
      "Epoch 271/1000\n",
      "1168/1168 [==============================] - 0s 159us/step - loss: 26340.8886 - val_loss: 41460.5078\n",
      "Epoch 272/1000\n",
      "1168/1168 [==============================] - 0s 214us/step - loss: 25919.4045 - val_loss: 42437.2700\n",
      "Epoch 273/1000\n",
      "1168/1168 [==============================] - 0s 132us/step - loss: 25440.0457 - val_loss: 42203.9693\n",
      "Epoch 274/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 25488.2364 - val_loss: 41587.7806\n",
      "Epoch 275/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 24988.4350 - val_loss: 41271.7251\n",
      "Epoch 276/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 26238.0480 - val_loss: 41529.6233\n",
      "Epoch 277/1000\n",
      "1168/1168 [==============================] - 0s 146us/step - loss: 25501.7537 - val_loss: 41422.6115\n",
      "Epoch 278/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 25403.2998 - val_loss: 41535.2694\n",
      "Epoch 279/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 25243.4312 - val_loss: 41800.0122\n",
      "Epoch 280/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 25662.3052 - val_loss: 42303.7049\n",
      "Epoch 281/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 26002.9264 - val_loss: 40789.3749\n",
      "Epoch 282/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 24841.8695 - val_loss: 41732.8572\n",
      "Epoch 283/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 25443.9846 - val_loss: 40954.6826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 25441.8139 - val_loss: 41743.8386\n",
      "Epoch 285/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 25499.4466 - val_loss: 41647.9530\n",
      "Epoch 286/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 24865.7208 - val_loss: 42503.1388\n",
      "Epoch 287/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 25935.4325 - val_loss: 41476.9532\n",
      "Epoch 288/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 25246.0431 - val_loss: 41640.4653\n",
      "Epoch 289/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 25320.5312 - val_loss: 41461.9454\n",
      "Epoch 290/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 24989.8959 - val_loss: 40868.8450\n",
      "Epoch 291/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 25508.6520 - val_loss: 40947.7241\n",
      "Epoch 292/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 25090.4101 - val_loss: 42906.5388\n",
      "Epoch 293/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 24926.3292 - val_loss: 40932.2019\n",
      "Epoch 294/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 24732.2213 - val_loss: 41085.1866\n",
      "Epoch 295/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 25340.8543 - val_loss: 41251.9557\n",
      "Epoch 296/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 25007.2033 - val_loss: 40315.2457\n",
      "Epoch 297/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 25060.2714 - val_loss: 41104.4912\n",
      "Epoch 298/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 25258.8202 - val_loss: 41231.4464\n",
      "Epoch 299/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 24819.9845 - val_loss: 41617.8664\n",
      "Epoch 300/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 24906.9474 - val_loss: 40767.5408\n",
      "Epoch 301/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 24614.6365 - val_loss: 41534.2330\n",
      "Epoch 302/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 24534.3804 - val_loss: 41171.5490\n",
      "Epoch 303/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 24893.2772 - val_loss: 40738.1354\n",
      "Epoch 304/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 26314.4881 - val_loss: 41228.7820\n",
      "Epoch 305/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 24639.6267 - val_loss: 42101.5125\n",
      "Epoch 306/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 24743.6165 - val_loss: 40039.7676\n",
      "Epoch 307/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 25317.7324 - val_loss: 41824.8156\n",
      "Epoch 308/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 24457.1534 - val_loss: 40451.8062\n",
      "Epoch 309/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 24362.9116 - val_loss: 42347.8835\n",
      "Epoch 310/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 24205.2001 - val_loss: 41077.2952\n",
      "Epoch 311/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 24241.9362 - val_loss: 41111.3352\n",
      "Epoch 312/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 24119.4582 - val_loss: 40532.0406\n",
      "Epoch 313/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 24200.8020 - val_loss: 40306.1674\n",
      "Epoch 314/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 24059.8850 - val_loss: 40066.9218\n",
      "Epoch 315/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 25190.0913 - val_loss: 40689.8842\n",
      "Epoch 316/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 25073.4668 - val_loss: 41261.1696\n",
      "Epoch 317/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 24281.5846 - val_loss: 40328.3410\n",
      "Epoch 318/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 24274.6173 - val_loss: 40390.0601\n",
      "Epoch 319/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 24356.7068 - val_loss: 41890.9237\n",
      "Epoch 320/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 24859.9398 - val_loss: 43643.8676\n",
      "Epoch 321/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 24319.1629 - val_loss: 39931.7793\n",
      "Epoch 322/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 24239.6127 - val_loss: 39929.7738\n",
      "Epoch 323/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 24946.6619 - val_loss: 40674.1362\n",
      "Epoch 324/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 24344.4258 - val_loss: 39999.4197\n",
      "Epoch 325/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 24116.9128 - val_loss: 42531.0506\n",
      "Epoch 326/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 23900.8581 - val_loss: 40865.5209\n",
      "Epoch 327/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 23965.8668 - val_loss: 40388.4994\n",
      "Epoch 328/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 23555.6121 - val_loss: 40853.0970\n",
      "Epoch 329/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 24209.2080 - val_loss: 39780.4730\n",
      "Epoch 330/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 24305.2877 - val_loss: 40708.8984\n",
      "Epoch 331/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 23953.2843 - val_loss: 41058.2793\n",
      "Epoch 332/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 23985.6930 - val_loss: 39749.4174\n",
      "Epoch 333/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 23914.1438 - val_loss: 40454.8257\n",
      "Epoch 334/1000\n",
      "1168/1168 [==============================] - 0s 146us/step - loss: 24142.0072 - val_loss: 39839.5230\n",
      "Epoch 335/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 23919.3462 - val_loss: 39635.1087\n",
      "Epoch 336/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 24062.6623 - val_loss: 40887.3309\n",
      "Epoch 337/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 23409.4334 - val_loss: 43272.0630\n",
      "Epoch 338/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 24522.3909 - val_loss: 39567.5784\n",
      "Epoch 339/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 23604.2604 - val_loss: 40681.8559\n",
      "Epoch 340/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 23907.1560 - val_loss: 40395.9322\n",
      "Epoch 341/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 23893.1518 - val_loss: 39331.2018\n",
      "Epoch 342/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 23702.2492 - val_loss: 39901.4952\n",
      "Epoch 343/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 23655.5071 - val_loss: 39221.1501\n",
      "Epoch 344/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 23601.4646 - val_loss: 39448.8891\n",
      "Epoch 345/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 23426.7818 - val_loss: 40232.3989\n",
      "Epoch 346/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 24077.2679 - val_loss: 39694.3353\n",
      "Epoch 347/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 23303.1863 - val_loss: 39600.0391\n",
      "Epoch 348/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 23206.8366 - val_loss: 41403.4715\n",
      "Epoch 349/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 23739.7167 - val_loss: 39614.7122\n",
      "Epoch 350/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 23609.2852 - val_loss: 40109.0166\n",
      "Epoch 351/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 23385.4033 - val_loss: 39341.5997\n",
      "Epoch 352/1000\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 23626.5356 - val_loss: 40870.6341\n",
      "Epoch 353/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 23649.5955 - val_loss: 40025.3696\n",
      "Epoch 354/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 23270.3669 - val_loss: 39102.9158\n",
      "Epoch 355/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 24073.8944 - val_loss: 38864.4704\n",
      "Epoch 356/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 23517.4339 - val_loss: 38883.4174\n",
      "Epoch 357/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 23404.2924 - val_loss: 40384.8032\n",
      "Epoch 358/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 23102.1072 - val_loss: 39245.1083\n",
      "Epoch 359/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 23271.2894 - val_loss: 39940.5704\n",
      "Epoch 360/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 23441.8867 - val_loss: 38870.4152\n",
      "Epoch 361/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 22595.3806 - val_loss: 39519.2862\n",
      "Epoch 362/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 23477.8725 - val_loss: 39690.5495\n",
      "Epoch 363/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 23496.4014 - val_loss: 38979.1552\n",
      "Epoch 364/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 22960.5874 - val_loss: 39447.0403\n",
      "Epoch 365/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 23647.7272 - val_loss: 39885.8737\n",
      "Epoch 366/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 23378.1160 - val_loss: 38883.2269\n",
      "Epoch 367/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 22916.7725 - val_loss: 39771.1300\n",
      "Epoch 368/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 23116.3306 - val_loss: 39168.6757\n",
      "Epoch 369/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 23151.1288 - val_loss: 39246.7336\n",
      "Epoch 370/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 23029.2922 - val_loss: 39577.9520\n",
      "Epoch 371/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 23117.5643 - val_loss: 40488.2889\n",
      "Epoch 372/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 23022.7617 - val_loss: 41280.3969\n",
      "Epoch 373/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 23356.7790 - val_loss: 41557.3022\n",
      "Epoch 374/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 22743.2024 - val_loss: 39361.6042\n",
      "Epoch 375/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 22823.8024 - val_loss: 39327.1450\n",
      "Epoch 376/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 23134.5318 - val_loss: 38623.2458\n",
      "Epoch 377/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 23075.4439 - val_loss: 38969.1336\n",
      "Epoch 378/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 23032.9944 - val_loss: 41317.3961\n",
      "Epoch 379/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 23152.4290 - val_loss: 38883.8433\n",
      "Epoch 380/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 22745.6295 - val_loss: 38240.5731\n",
      "Epoch 381/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 23293.6471 - val_loss: 39093.9536\n",
      "Epoch 382/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 22755.5896 - val_loss: 41375.3286\n",
      "Epoch 383/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 22617.0126 - val_loss: 42682.8219\n",
      "Epoch 384/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 23133.3558 - val_loss: 39029.1408\n",
      "Epoch 385/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 22650.3967 - val_loss: 41604.7587\n",
      "Epoch 386/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 22872.4609 - val_loss: 39292.3459\n",
      "Epoch 387/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 22784.0720 - val_loss: 39269.6504\n",
      "Epoch 388/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 22719.4256 - val_loss: 39149.6944\n",
      "Epoch 389/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 23329.9434 - val_loss: 38861.2519\n",
      "Epoch 390/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 22565.5365 - val_loss: 39081.9237\n",
      "Epoch 391/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 22431.1759 - val_loss: 38539.4638\n",
      "Epoch 392/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 22575.1046 - val_loss: 39068.2739\n",
      "Epoch 393/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 22637.5056 - val_loss: 39152.8935\n",
      "Epoch 394/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 22778.1725 - val_loss: 40335.6546\n",
      "Epoch 395/1000\n",
      "1168/1168 [==============================] - 0s 108us/step - loss: 22397.7511 - val_loss: 38854.2703\n",
      "Epoch 396/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 23085.6093 - val_loss: 38442.7429\n",
      "Epoch 397/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 22444.9654 - val_loss: 39118.3795\n",
      "Epoch 398/1000\n",
      "1168/1168 [==============================] - 0s 107us/step - loss: 22665.3667 - val_loss: 38422.5484\n",
      "Epoch 399/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 22653.8215 - val_loss: 38245.9043\n",
      "Epoch 400/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 22451.5009 - val_loss: 38203.8854\n",
      "Epoch 401/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 22532.4646 - val_loss: 38237.2842\n",
      "Epoch 402/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 22408.6248 - val_loss: 38787.8486\n",
      "Epoch 403/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 21779.0992 - val_loss: 38907.3291\n",
      "Epoch 404/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 22409.0390 - val_loss: 40021.4573\n",
      "Epoch 405/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 22269.0454 - val_loss: 40249.7636\n",
      "Epoch 406/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 22275.9751 - val_loss: 40089.6514\n",
      "Epoch 407/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 22187.9657 - val_loss: 39225.0865\n",
      "Epoch 408/1000\n",
      "1168/1168 [==============================] - 0s 108us/step - loss: 22040.5064 - val_loss: 38520.6242\n",
      "Epoch 409/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 22016.0087 - val_loss: 40302.1312\n",
      "Epoch 410/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 22767.6832 - val_loss: 38455.0338\n",
      "Epoch 411/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 21846.2960 - val_loss: 38076.7089\n",
      "Epoch 412/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 21978.2786 - val_loss: 38483.0206\n",
      "Epoch 413/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 22152.4725 - val_loss: 38672.5003\n",
      "Epoch 414/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 22015.4396 - val_loss: 38329.4314\n",
      "Epoch 415/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 22340.7334 - val_loss: 38438.6745\n",
      "Epoch 416/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 22218.3453 - val_loss: 38965.7115\n",
      "Epoch 417/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 21888.8587 - val_loss: 38516.1894\n",
      "Epoch 418/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 22701.8732 - val_loss: 41626.6945\n",
      "Epoch 419/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 22366.6735 - val_loss: 37899.1700\n",
      "Epoch 420/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 22385.2107 - val_loss: 42825.8165\n",
      "Epoch 421/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 22148.8341 - val_loss: 38250.4779\n",
      "Epoch 422/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 21710.4217 - val_loss: 40283.2444\n",
      "Epoch 423/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 22333.2170 - val_loss: 38730.4301\n",
      "Epoch 424/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 21585.6404 - val_loss: 38640.3909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 425/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 22120.5049 - val_loss: 38330.0175\n",
      "Epoch 426/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 22042.4809 - val_loss: 38248.6859\n",
      "Epoch 427/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 21908.8843 - val_loss: 38166.5297\n",
      "Epoch 428/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 22334.9852 - val_loss: 38350.1638\n",
      "Epoch 429/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 21836.2116 - val_loss: 37756.9520\n",
      "Epoch 430/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 22239.2455 - val_loss: 37932.7414\n",
      "Epoch 431/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 21743.1284 - val_loss: 41128.3715\n",
      "Epoch 432/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 21420.9792 - val_loss: 37954.9181\n",
      "Epoch 433/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 21920.6997 - val_loss: 38606.5899\n",
      "Epoch 434/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 21399.6002 - val_loss: 39377.4034\n",
      "Epoch 435/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 21823.6952 - val_loss: 38339.0215\n",
      "Epoch 436/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 21513.0821 - val_loss: 38161.1350\n",
      "Epoch 437/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 21880.3299 - val_loss: 40738.3403\n",
      "Epoch 438/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 21553.5813 - val_loss: 38096.3471\n",
      "Epoch 439/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 21525.9394 - val_loss: 37418.4294\n",
      "Epoch 440/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 21465.5593 - val_loss: 38414.0443\n",
      "Epoch 441/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 21825.1267 - val_loss: 38172.1105\n",
      "Epoch 442/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 21408.3819 - val_loss: 37911.6064\n",
      "Epoch 443/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 21765.1119 - val_loss: 39537.0753\n",
      "Epoch 444/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 21486.9817 - val_loss: 37611.2027\n",
      "Epoch 445/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 21883.8283 - val_loss: 37795.6860\n",
      "Epoch 446/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 21970.2256 - val_loss: 37935.6258\n",
      "Epoch 447/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 21403.6367 - val_loss: 38630.7997\n",
      "Epoch 448/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 21138.4760 - val_loss: 38163.1037\n",
      "Epoch 449/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 21390.7228 - val_loss: 41911.2970\n",
      "Epoch 450/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 21821.5309 - val_loss: 37721.7306\n",
      "Epoch 451/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 21523.5800 - val_loss: 38479.0252\n",
      "Epoch 452/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 21474.7488 - val_loss: 38698.5006\n",
      "Epoch 453/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 22048.9312 - val_loss: 38064.8220\n",
      "Epoch 454/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 21472.7590 - val_loss: 37645.5980\n",
      "Epoch 455/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 20928.0541 - val_loss: 39517.0840\n",
      "Epoch 456/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 21527.5495 - val_loss: 38115.8640\n",
      "Epoch 457/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 21703.2306 - val_loss: 37936.8655\n",
      "Epoch 458/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 20955.2329 - val_loss: 39186.7493\n",
      "Epoch 459/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 21197.1159 - val_loss: 38239.3043\n",
      "Epoch 460/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 21436.5958 - val_loss: 37312.9764\n",
      "Epoch 461/1000\n",
      "1168/1168 [==============================] - 0s 108us/step - loss: 21287.9721 - val_loss: 38074.9792\n",
      "Epoch 462/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 21074.9842 - val_loss: 38633.8816\n",
      "Epoch 463/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 20688.7893 - val_loss: 39302.9114\n",
      "Epoch 464/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 21915.3305 - val_loss: 37540.7696\n",
      "Epoch 465/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 21056.8520 - val_loss: 37349.3525\n",
      "Epoch 466/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 21224.3776 - val_loss: 39171.0649\n",
      "Epoch 467/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 20781.1530 - val_loss: 38465.1271\n",
      "Epoch 468/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 21248.4930 - val_loss: 39052.1487\n",
      "Epoch 469/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 21227.7699 - val_loss: 37532.5662\n",
      "Epoch 470/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 21537.7892 - val_loss: 38025.5400\n",
      "Epoch 471/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 21513.7954 - val_loss: 37174.4510\n",
      "Epoch 472/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 20790.8610 - val_loss: 37618.6481\n",
      "Epoch 473/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 20755.1405 - val_loss: 37297.5762\n",
      "Epoch 474/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 20982.7606 - val_loss: 37768.2167\n",
      "Epoch 475/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 21039.5716 - val_loss: 39039.8090\n",
      "Epoch 476/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 21340.8508 - val_loss: 38029.4922\n",
      "Epoch 477/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 21332.5786 - val_loss: 37605.6231\n",
      "Epoch 478/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 21199.2114 - val_loss: 37763.7283\n",
      "Epoch 479/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 21301.0004 - val_loss: 37195.2641\n",
      "Epoch 480/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 21024.1443 - val_loss: 37734.9654\n",
      "Epoch 481/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 21195.4012 - val_loss: 39224.3717\n",
      "Epoch 482/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 21369.9529 - val_loss: 37588.2472\n",
      "Epoch 483/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 21072.4510 - val_loss: 37406.4256\n",
      "Epoch 484/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 20915.4685 - val_loss: 37199.2953\n",
      "Epoch 485/1000\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 21355.6695 - val_loss: 38071.2183\n",
      "Epoch 486/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 20836.4222 - val_loss: 36986.7052\n",
      "Epoch 487/1000\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 20776.5247 - val_loss: 37799.6552\n",
      "Epoch 488/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 21139.5338 - val_loss: 37096.7552\n",
      "Epoch 489/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 21090.3840 - val_loss: 37980.4328\n",
      "Epoch 490/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 21496.8155 - val_loss: 37304.7204\n",
      "Epoch 491/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 20955.0135 - val_loss: 37465.7215\n",
      "Epoch 492/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 20806.6990 - val_loss: 37225.3686\n",
      "Epoch 493/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 20738.9690 - val_loss: 38156.8464\n",
      "Epoch 494/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 21476.2224 - val_loss: 38894.4516\n",
      "Epoch 495/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 20339.0075 - val_loss: 36977.5775\n",
      "Epoch 496/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 20444.8187 - val_loss: 36971.4927\n",
      "Epoch 497/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 20932.9373 - val_loss: 37964.9151\n",
      "Epoch 498/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 21002.9750 - val_loss: 37087.6695\n",
      "Epoch 499/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 20934.1132 - val_loss: 37464.6032\n",
      "Epoch 500/1000\n",
      "1168/1168 [==============================] - 0s 143us/step - loss: 20697.2731 - val_loss: 37649.6874\n",
      "Epoch 501/1000\n",
      "1168/1168 [==============================] - 0s 142us/step - loss: 21046.7557 - val_loss: 36612.4216\n",
      "Epoch 502/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 20135.7360 - val_loss: 37267.3177\n",
      "Epoch 503/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 20475.9130 - val_loss: 36760.2628\n",
      "Epoch 504/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 20743.1458 - val_loss: 36972.4495\n",
      "Epoch 505/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 20283.5219 - val_loss: 35914.7141\n",
      "Epoch 506/1000\n",
      "1168/1168 [==============================] - 0s 108us/step - loss: 19898.9763 - val_loss: 37849.0402\n",
      "Epoch 507/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 20222.5077 - val_loss: 36781.0214\n",
      "Epoch 508/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 20325.5390 - val_loss: 38436.4619\n",
      "Epoch 509/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 20370.6793 - val_loss: 36730.5514\n",
      "Epoch 510/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 20968.8480 - val_loss: 40309.2115\n",
      "Epoch 511/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 20753.8607 - val_loss: 36983.0107\n",
      "Epoch 512/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 20581.6153 - val_loss: 37156.4941\n",
      "Epoch 513/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 20004.0663 - val_loss: 36652.7551\n",
      "Epoch 514/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 19874.3349 - val_loss: 36933.6600\n",
      "Epoch 515/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 20429.2547 - val_loss: 37975.1945\n",
      "Epoch 516/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 20303.4317 - val_loss: 37432.6277\n",
      "Epoch 517/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 20412.1094 - val_loss: 37651.1196\n",
      "Epoch 518/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 20602.9912 - val_loss: 38238.5283\n",
      "Epoch 519/1000\n",
      "1168/1168 [==============================] - 0s 146us/step - loss: 20410.8056 - val_loss: 37026.5180\n",
      "Epoch 520/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 20340.1106 - val_loss: 36820.7631\n",
      "Epoch 521/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 20524.5496 - val_loss: 38603.4414\n",
      "Epoch 522/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 20431.1984 - val_loss: 37041.5151\n",
      "Epoch 523/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 20217.9961 - val_loss: 37342.4446\n",
      "Epoch 524/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 19943.2605 - val_loss: 39668.0698\n",
      "Epoch 525/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 19860.0675 - val_loss: 37185.6608\n",
      "Epoch 526/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 20171.8487 - val_loss: 36926.6831\n",
      "Epoch 527/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 20207.0714 - val_loss: 36671.1610\n",
      "Epoch 528/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 20169.0679 - val_loss: 36813.5757\n",
      "Epoch 529/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 19896.1283 - val_loss: 36789.7422\n",
      "Epoch 530/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 20162.7750 - val_loss: 37015.5854\n",
      "Epoch 531/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 20672.8622 - val_loss: 37695.7722\n",
      "Epoch 532/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 20042.9030 - val_loss: 38296.1550\n",
      "Epoch 533/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 20493.1040 - val_loss: 38435.6874\n",
      "Epoch 534/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 20048.5769 - val_loss: 37216.7479\n",
      "Epoch 535/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 20027.2525 - val_loss: 37711.3243\n",
      "Epoch 536/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 20469.5856 - val_loss: 36861.8106\n",
      "Epoch 537/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 20121.2985 - val_loss: 36834.2416\n",
      "Epoch 538/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 20543.4028 - val_loss: 40453.3564\n",
      "Epoch 539/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 20554.9317 - val_loss: 36968.6290\n",
      "Epoch 540/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 20354.7134 - val_loss: 36732.0858\n",
      "Epoch 541/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 20438.1436 - val_loss: 38246.4330\n",
      "Epoch 542/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 20725.7080 - val_loss: 36638.5592\n",
      "Epoch 543/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 19933.3026 - val_loss: 37186.1778\n",
      "Epoch 544/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 20415.6627 - val_loss: 36524.0761\n",
      "Epoch 545/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 19963.7284 - val_loss: 36494.8258\n",
      "Epoch 546/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 20045.6035 - val_loss: 36874.3580\n",
      "Epoch 547/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 19935.4609 - val_loss: 36560.5883\n",
      "Epoch 548/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 19757.8534 - val_loss: 37328.2975\n",
      "Epoch 549/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 20207.1811 - val_loss: 38469.4257\n",
      "Epoch 550/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 19894.7467 - val_loss: 36754.3051\n",
      "Epoch 551/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 20037.4773 - val_loss: 36673.1930\n",
      "Epoch 552/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 19922.3711 - val_loss: 36705.5105\n",
      "Epoch 553/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 19750.5935 - val_loss: 38222.7399\n",
      "Epoch 554/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 20644.7755 - val_loss: 37742.8354\n",
      "Epoch 555/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 20894.3606 - val_loss: 36377.1647\n",
      "Epoch 556/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 19809.3159 - val_loss: 36547.9578\n",
      "Epoch 557/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 19999.3980 - val_loss: 36011.3457\n",
      "Epoch 558/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 19852.7801 - val_loss: 36490.0717\n",
      "Epoch 559/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 19277.6039 - val_loss: 41024.5592\n",
      "Epoch 560/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 20254.3999 - val_loss: 37227.3082\n",
      "Epoch 561/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 20396.6250 - val_loss: 36759.0929\n",
      "Epoch 562/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 19698.8796 - val_loss: 38292.5576\n",
      "Epoch 563/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 20681.3705 - val_loss: 36567.8608\n",
      "Epoch 564/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 19938.7080 - val_loss: 36419.3186\n",
      "Epoch 565/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 19763.2050 - val_loss: 36538.5951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 566/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 19838.7803 - val_loss: 36416.0669\n",
      "Epoch 567/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 20082.1720 - val_loss: 38577.7738\n",
      "Epoch 568/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 20379.3133 - val_loss: 36202.1502\n",
      "Epoch 569/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 19628.4090 - val_loss: 35675.1302\n",
      "Epoch 570/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 19737.9573 - val_loss: 36288.1479\n",
      "Epoch 571/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 19678.2971 - val_loss: 36668.3182\n",
      "Epoch 572/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 19928.7119 - val_loss: 38562.0660\n",
      "Epoch 573/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 20256.1911 - val_loss: 36766.0752\n",
      "Epoch 574/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 19602.0429 - val_loss: 36734.3624\n",
      "Epoch 575/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 19856.8917 - val_loss: 36303.5193\n",
      "Epoch 576/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 19549.0131 - val_loss: 36569.7969\n",
      "Epoch 577/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 19927.9560 - val_loss: 37616.3255\n",
      "Epoch 578/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 19768.9685 - val_loss: 36348.5432\n",
      "Epoch 579/1000\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 19834.0096 - val_loss: 36506.9916\n",
      "Epoch 580/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 19697.3886 - val_loss: 36287.0008\n",
      "Epoch 581/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 19323.7814 - val_loss: 36328.7401\n",
      "Epoch 582/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 19176.2859 - val_loss: 35705.5767\n",
      "Epoch 583/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 20040.6088 - val_loss: 35998.6544\n",
      "Epoch 584/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 19484.3560 - val_loss: 36059.7408\n",
      "Epoch 585/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 19895.5940 - val_loss: 37773.9687\n",
      "Epoch 586/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 19304.5659 - val_loss: 36357.8811\n",
      "Epoch 587/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 19276.3272 - val_loss: 36189.6343\n",
      "Epoch 588/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 19790.2712 - val_loss: 36881.1236\n",
      "Epoch 589/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 19600.6985 - val_loss: 43171.9296\n",
      "Epoch 590/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 19586.8799 - val_loss: 36500.7106\n",
      "Epoch 591/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 19617.1923 - val_loss: 38432.0720\n",
      "Epoch 592/1000\n",
      "1168/1168 [==============================] - 0s 106us/step - loss: 19345.5510 - val_loss: 35719.8194\n",
      "Epoch 593/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 19892.2341 - val_loss: 36378.3079\n",
      "Epoch 594/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 20241.9455 - val_loss: 38293.5889\n",
      "Epoch 595/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 20267.2906 - val_loss: 36597.2141\n",
      "Epoch 596/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 19331.6514 - val_loss: 35769.2419\n",
      "Epoch 597/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 20141.7330 - val_loss: 36340.2403\n",
      "Epoch 598/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 19766.9641 - val_loss: 35933.8289\n",
      "Epoch 599/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 19008.4192 - val_loss: 36182.4856\n",
      "Epoch 600/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 19793.2499 - val_loss: 35669.0505\n",
      "Epoch 601/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 18871.1645 - val_loss: 36756.3697\n",
      "Epoch 602/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 19330.9296 - val_loss: 35851.8652\n",
      "Epoch 603/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 19940.6884 - val_loss: 36461.9965\n",
      "Epoch 604/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 19258.6982 - val_loss: 35769.1617\n",
      "Epoch 605/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 19511.5849 - val_loss: 35832.8422\n",
      "Epoch 606/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 19396.6961 - val_loss: 36321.5537\n",
      "Epoch 607/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 19452.0870 - val_loss: 35779.8043\n",
      "Epoch 608/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 19108.1727 - val_loss: 35682.8004\n",
      "Epoch 609/1000\n",
      "1168/1168 [==============================] - 0s 108us/step - loss: 19011.5213 - val_loss: 40092.2152\n",
      "Epoch 610/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 19702.4298 - val_loss: 38163.6848\n",
      "Epoch 611/1000\n",
      "1168/1168 [==============================] - 0s 132us/step - loss: 19210.9298 - val_loss: 36588.7430\n",
      "Epoch 612/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 19377.3106 - val_loss: 36354.9202\n",
      "Epoch 613/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 19208.8378 - val_loss: 36245.8925\n",
      "Epoch 614/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 19760.0901 - val_loss: 35954.1311\n",
      "Epoch 615/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 19747.2225 - val_loss: 40073.6866\n",
      "Epoch 616/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 19242.7210 - val_loss: 36692.6360\n",
      "Epoch 617/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 19346.7921 - val_loss: 35235.2011\n",
      "Epoch 618/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 18647.4194 - val_loss: 36654.7928\n",
      "Epoch 619/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 19142.4367 - val_loss: 35716.7573\n",
      "Epoch 620/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 19290.1591 - val_loss: 35471.8095\n",
      "Epoch 621/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 20125.8367 - val_loss: 37063.9325\n",
      "Epoch 622/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 19472.6326 - val_loss: 36657.1521\n",
      "Epoch 623/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 19107.0499 - val_loss: 35450.1872\n",
      "Epoch 624/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 19155.9406 - val_loss: 35257.8213\n",
      "Epoch 625/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 19101.9937 - val_loss: 35936.5680\n",
      "Epoch 626/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 18898.6082 - val_loss: 40047.5773\n",
      "Epoch 627/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 19618.3754 - val_loss: 35500.9874\n",
      "Epoch 628/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 19041.1393 - val_loss: 37219.9455\n",
      "Epoch 629/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 18972.7945 - val_loss: 35414.9744\n",
      "Epoch 630/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 19640.6424 - val_loss: 36073.1119\n",
      "Epoch 631/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 19289.5153 - val_loss: 35749.7853\n",
      "Epoch 632/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 19220.8647 - val_loss: 35089.1145\n",
      "Epoch 633/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 18897.8695 - val_loss: 35970.6839\n",
      "Epoch 634/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 18799.9799 - val_loss: 35703.2165\n",
      "Epoch 635/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 18993.4853 - val_loss: 39191.3721\n",
      "Epoch 636/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 18782.6568 - val_loss: 36438.7868\n",
      "Epoch 637/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 18982.2818 - val_loss: 36397.8955\n",
      "Epoch 638/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 19362.1950 - val_loss: 36224.0909\n",
      "Epoch 639/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 19364.1260 - val_loss: 36989.5561\n",
      "Epoch 640/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 18720.8766 - val_loss: 35645.7334\n",
      "Epoch 641/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 18964.9361 - val_loss: 35480.2988\n",
      "Epoch 642/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 18395.8165 - val_loss: 35563.3254\n",
      "Epoch 643/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 18624.6902 - val_loss: 38318.0463\n",
      "Epoch 644/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 19289.5359 - val_loss: 36103.5191\n",
      "Epoch 645/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 20806.1356 - val_loss: 36557.3603\n",
      "Epoch 646/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 19242.8349 - val_loss: 35512.3426\n",
      "Epoch 647/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 19750.5243 - val_loss: 35648.8323\n",
      "Epoch 648/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 18859.0974 - val_loss: 35467.3418\n",
      "Epoch 649/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 19054.1840 - val_loss: 35568.0596\n",
      "Epoch 650/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 18921.9048 - val_loss: 37644.5743\n",
      "Epoch 651/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 18667.2329 - val_loss: 36593.0264\n",
      "Epoch 652/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 18711.8484 - val_loss: 34920.9999\n",
      "Epoch 653/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 18861.1758 - val_loss: 35228.0996\n",
      "Epoch 654/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 18879.2959 - val_loss: 35644.6600\n",
      "Epoch 655/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 18871.3363 - val_loss: 35107.6664\n",
      "Epoch 656/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 19101.5513 - val_loss: 35320.6251\n",
      "Epoch 657/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 18837.8328 - val_loss: 36011.1492\n",
      "Epoch 658/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 18589.7256 - val_loss: 35245.1198\n",
      "Epoch 659/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 18620.2742 - val_loss: 35109.1410\n",
      "Epoch 660/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 18338.8469 - val_loss: 35902.2318\n",
      "Epoch 661/1000\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 18698.8014 - val_loss: 36624.0868\n",
      "Epoch 662/1000\n",
      "1168/1168 [==============================] - 0s 198us/step - loss: 18763.4577 - val_loss: 37277.4624\n",
      "Epoch 663/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 18688.8415 - val_loss: 36173.5360\n",
      "Epoch 664/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 18772.6805 - val_loss: 37502.9605\n",
      "Epoch 665/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 18663.1190 - val_loss: 35336.9324\n",
      "Epoch 666/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 19377.9154 - val_loss: 35322.2699\n",
      "Epoch 667/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 18752.2955 - val_loss: 35054.4806\n",
      "Epoch 668/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 18121.1915 - val_loss: 36272.2344\n",
      "Epoch 669/1000\n",
      "1168/1168 [==============================] - 0s 106us/step - loss: 18695.6498 - val_loss: 35194.7397\n",
      "Epoch 670/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 18787.7277 - val_loss: 35422.0023\n",
      "Epoch 671/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 18701.3329 - val_loss: 35426.1129\n",
      "Epoch 672/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 18833.0399 - val_loss: 36257.0388\n",
      "Epoch 673/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 18603.9407 - val_loss: 37301.5067\n",
      "Epoch 674/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 18762.4869 - val_loss: 35479.7572\n",
      "Epoch 675/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 18761.5371 - val_loss: 37729.5967\n",
      "Epoch 676/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 18697.9003 - val_loss: 35019.2532\n",
      "Epoch 677/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 18629.0478 - val_loss: 35044.7261\n",
      "Epoch 678/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 19791.5579 - val_loss: 36823.2815\n",
      "Epoch 679/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 18659.5961 - val_loss: 35743.2512\n",
      "Epoch 680/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 18661.4883 - val_loss: 35276.4289\n",
      "Epoch 681/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 18537.9055 - val_loss: 34908.5060\n",
      "Epoch 682/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 18938.9443 - val_loss: 37285.8652\n",
      "Epoch 683/1000\n",
      "1168/1168 [==============================] - 0s 137us/step - loss: 18370.5530 - val_loss: 35078.7792\n",
      "Epoch 684/1000\n",
      "1168/1168 [==============================] - 0s 202us/step - loss: 18788.2527 - val_loss: 35174.9078\n",
      "Epoch 685/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 18224.6059 - val_loss: 35048.5430\n",
      "Epoch 686/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 18540.0713 - val_loss: 34659.0414\n",
      "Epoch 687/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 18796.1821 - val_loss: 36010.9313\n",
      "Epoch 688/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 18620.7845 - val_loss: 34726.3012\n",
      "Epoch 689/1000\n",
      "1168/1168 [==============================] - 0s 136us/step - loss: 18356.7215 - val_loss: 36044.3199\n",
      "Epoch 690/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 18367.2134 - val_loss: 36226.7668\n",
      "Epoch 691/1000\n",
      "1168/1168 [==============================] - 0s 163us/step - loss: 18086.6529 - val_loss: 35373.1973\n",
      "Epoch 692/1000\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 18833.4991 - val_loss: 36315.5956\n",
      "Epoch 693/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 18743.3556 - val_loss: 35189.7777\n",
      "Epoch 694/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 18999.2875 - val_loss: 35486.3035\n",
      "Epoch 695/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 18971.4032 - val_loss: 34658.8008\n",
      "Epoch 696/1000\n",
      "1168/1168 [==============================] - 0s 140us/step - loss: 18234.1368 - val_loss: 35145.1946\n",
      "Epoch 697/1000\n",
      "1168/1168 [==============================] - 0s 132us/step - loss: 18776.7805 - val_loss: 35001.6472\n",
      "Epoch 698/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 18702.6943 - val_loss: 35507.2727\n",
      "Epoch 699/1000\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 18877.2860 - val_loss: 38752.5225\n",
      "Epoch 700/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 18374.0612 - val_loss: 35521.9338\n",
      "Epoch 701/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 18379.0369 - val_loss: 36538.5911\n",
      "Epoch 702/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 18648.2143 - val_loss: 35050.4820\n",
      "Epoch 703/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 18234.4644 - val_loss: 36847.2671\n",
      "Epoch 704/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 18678.9463 - val_loss: 36986.6040\n",
      "Epoch 705/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 18814.1654 - val_loss: 35386.6066\n",
      "Epoch 706/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 18988.4025 - val_loss: 35727.1414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 707/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 18221.8132 - val_loss: 35484.9549\n",
      "Epoch 708/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 18391.0992 - val_loss: 35942.4580\n",
      "Epoch 709/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 17942.5518 - val_loss: 34953.8227\n",
      "Epoch 710/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 18265.0408 - val_loss: 34540.1335\n",
      "Epoch 711/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 17819.5543 - val_loss: 35164.2948\n",
      "Epoch 712/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 18337.9296 - val_loss: 35055.0301\n",
      "Epoch 713/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 18179.3534 - val_loss: 36443.3066\n",
      "Epoch 714/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 18378.7428 - val_loss: 35672.0521\n",
      "Epoch 715/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 18169.8358 - val_loss: 35782.5947\n",
      "Epoch 716/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 18827.6444 - val_loss: 36908.0190\n",
      "Epoch 717/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 18374.5322 - val_loss: 36148.6727\n",
      "Epoch 718/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 17890.3835 - val_loss: 35465.7165\n",
      "Epoch 719/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 18404.4453 - val_loss: 34263.5237\n",
      "Epoch 720/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 18361.2040 - val_loss: 34910.5621\n",
      "Epoch 721/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 18023.4726 - val_loss: 35342.2045\n",
      "Epoch 722/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 18164.9375 - val_loss: 35634.9227\n",
      "Epoch 723/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 18257.2054 - val_loss: 34819.6212\n",
      "Epoch 724/1000\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 18065.1387 - val_loss: 35574.8807\n",
      "Epoch 725/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 17880.3716 - val_loss: 35120.4587\n",
      "Epoch 726/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 18241.8273 - val_loss: 34537.4361\n",
      "Epoch 727/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 18775.8838 - val_loss: 35203.7896\n",
      "Epoch 728/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 17930.6671 - val_loss: 34744.8658\n",
      "Epoch 729/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 18777.5044 - val_loss: 35905.8757\n",
      "Epoch 730/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 18275.5954 - val_loss: 35757.4119\n",
      "Epoch 731/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 17992.3809 - val_loss: 35044.8076\n",
      "Epoch 732/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 17906.9666 - val_loss: 34600.6586\n",
      "Epoch 733/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 18225.1333 - val_loss: 35600.1332\n",
      "Epoch 734/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 18094.8484 - val_loss: 38583.7125\n",
      "Epoch 735/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 18067.9017 - val_loss: 34671.2103\n",
      "Epoch 736/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 18032.2790 - val_loss: 35050.2653\n",
      "Epoch 737/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 18537.6708 - val_loss: 36868.6013\n",
      "Epoch 738/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 17904.6177 - val_loss: 34959.6381\n",
      "Epoch 739/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 19274.3802 - val_loss: 34561.1104\n",
      "Epoch 740/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 18477.9531 - val_loss: 37440.9570\n",
      "Epoch 741/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 18458.1782 - val_loss: 35256.9111\n",
      "Epoch 742/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 17699.8898 - val_loss: 34744.9367\n",
      "Epoch 743/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 18621.7219 - val_loss: 34744.7093\n",
      "Epoch 744/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 17763.8866 - val_loss: 34451.9368\n",
      "Epoch 745/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 18034.9012 - val_loss: 34267.4001\n",
      "Epoch 746/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 18191.6500 - val_loss: 34885.5022\n",
      "Epoch 747/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 17897.4646 - val_loss: 35348.5674\n",
      "Epoch 748/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 17760.2007 - val_loss: 37153.2289\n",
      "Epoch 749/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 17950.2644 - val_loss: 35466.1985\n",
      "Epoch 750/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 18429.7061 - val_loss: 35604.5105\n",
      "Epoch 751/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 17877.2139 - val_loss: 35232.3431\n",
      "Epoch 752/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 18424.6279 - val_loss: 35376.5231\n",
      "Epoch 753/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 18381.9878 - val_loss: 34940.5497\n",
      "Epoch 754/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 17488.1137 - val_loss: 36634.9968\n",
      "Epoch 755/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 18121.9789 - val_loss: 35259.6566\n",
      "Epoch 756/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 18340.1334 - val_loss: 35301.7070\n",
      "Epoch 757/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 17764.2312 - val_loss: 35089.1065\n",
      "Epoch 758/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 18478.7614 - val_loss: 35594.9442\n",
      "Epoch 759/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 18820.1321 - val_loss: 34803.8353\n",
      "Epoch 760/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 17586.4136 - val_loss: 35132.0612\n",
      "Epoch 761/1000\n",
      "1168/1168 [==============================] - 0s 143us/step - loss: 18232.8527 - val_loss: 34790.7181\n",
      "Epoch 762/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 17779.2097 - val_loss: 35733.5269\n",
      "Epoch 763/1000\n",
      "1168/1168 [==============================] - 0s 107us/step - loss: 17898.1384 - val_loss: 34474.8900\n",
      "Epoch 764/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 17445.7876 - val_loss: 34260.0555\n",
      "Epoch 765/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 18317.5857 - val_loss: 34722.7193\n",
      "Epoch 766/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 17971.1239 - val_loss: 35048.1466\n",
      "Epoch 767/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 18264.6227 - val_loss: 35568.9599\n",
      "Epoch 768/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 17893.4736 - val_loss: 35093.1925\n",
      "Epoch 769/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 18156.6121 - val_loss: 36258.7388\n",
      "Epoch 770/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 17749.6343 - val_loss: 34735.0331\n",
      "Epoch 771/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 17843.7859 - val_loss: 36447.6327\n",
      "Epoch 772/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 18107.6389 - val_loss: 35009.9619\n",
      "Epoch 773/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 17601.2047 - val_loss: 38850.4672\n",
      "Epoch 774/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 17508.0326 - val_loss: 34938.2271\n",
      "Epoch 775/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 17966.8604 - val_loss: 34928.4277\n",
      "Epoch 776/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 17777.8921 - val_loss: 34914.5317\n",
      "Epoch 777/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 17751.5804 - val_loss: 34565.0836\n",
      "Epoch 778/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 17356.0733 - val_loss: 33689.7834\n",
      "Epoch 779/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 17253.1043 - val_loss: 35160.8042\n",
      "Epoch 780/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 17107.7093 - val_loss: 34696.7556\n",
      "Epoch 781/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 17313.9735 - val_loss: 34520.1264\n",
      "Epoch 782/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 18920.1263 - val_loss: 34289.2216\n",
      "Epoch 783/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 17516.6920 - val_loss: 35544.9691\n",
      "Epoch 784/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 17275.9206 - val_loss: 35212.4263\n",
      "Epoch 785/1000\n",
      "1168/1168 [==============================] - 0s 148us/step - loss: 17609.0784 - val_loss: 35916.7228\n",
      "Epoch 786/1000\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 17411.9376 - val_loss: 35023.5779\n",
      "Epoch 787/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 17202.1876 - val_loss: 34413.9188\n",
      "Epoch 788/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 18133.5491 - val_loss: 37145.2289\n",
      "Epoch 789/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 19508.0543 - val_loss: 34594.3684\n",
      "Epoch 790/1000\n",
      "1168/1168 [==============================] - 0s 200us/step - loss: 17653.1495 - val_loss: 35169.5742\n",
      "Epoch 791/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 17932.9056 - val_loss: 38547.4536\n",
      "Epoch 792/1000\n",
      "1168/1168 [==============================] - 0s 107us/step - loss: 18720.3747 - val_loss: 34274.6839\n",
      "Epoch 793/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 17644.9479 - val_loss: 38593.8826\n",
      "Epoch 794/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 18397.9590 - val_loss: 34902.3836\n",
      "Epoch 795/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 18327.1589 - val_loss: 34410.0619\n",
      "Epoch 796/1000\n",
      "1168/1168 [==============================] - 0s 219us/step - loss: 17359.4067 - val_loss: 35627.6236\n",
      "Epoch 797/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 17668.2840 - val_loss: 34442.5447\n",
      "Epoch 798/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 17331.2831 - val_loss: 36564.8874\n",
      "Epoch 799/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 17706.6575 - val_loss: 35999.8411\n",
      "Epoch 800/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 17424.1165 - val_loss: 34438.9221\n",
      "Epoch 801/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 17464.2017 - val_loss: 34475.9833\n",
      "Epoch 802/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 19416.5419 - val_loss: 34685.3177\n",
      "Epoch 803/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 17459.5019 - val_loss: 35453.2965\n",
      "Epoch 804/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 17685.3849 - val_loss: 35886.3266\n",
      "Epoch 805/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 17363.9523 - val_loss: 34572.2115\n",
      "Epoch 806/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 17338.8759 - val_loss: 34304.1010\n",
      "Epoch 807/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 17811.5165 - val_loss: 36937.6015\n",
      "Epoch 808/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 18761.3876 - val_loss: 35577.8350\n",
      "Epoch 809/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 17972.5223 - val_loss: 35754.6985\n",
      "Epoch 810/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 17205.9805 - val_loss: 34399.0733\n",
      "Epoch 811/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 17666.7475 - val_loss: 34372.8535\n",
      "Epoch 812/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 17444.9861 - val_loss: 35264.2993\n",
      "Epoch 813/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 17631.7600 - val_loss: 34239.2697\n",
      "Epoch 814/1000\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 17374.6409 - val_loss: 34877.7991\n",
      "Epoch 815/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 17353.1150 - val_loss: 34414.8856\n",
      "Epoch 816/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 17788.2278 - val_loss: 34213.4823\n",
      "Epoch 817/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 17500.8031 - val_loss: 34231.8294\n",
      "Epoch 818/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 18302.4465 - val_loss: 35248.1753\n",
      "Epoch 819/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 17342.0389 - val_loss: 36266.4695\n",
      "Epoch 820/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 17593.4301 - val_loss: 35256.9661\n",
      "Epoch 821/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 17289.6674 - val_loss: 34465.6217\n",
      "Epoch 822/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 18249.3378 - val_loss: 38960.1244\n",
      "Epoch 823/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 17736.8639 - val_loss: 34900.7144\n",
      "Epoch 824/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 18295.1715 - val_loss: 33870.9451\n",
      "Epoch 825/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 17405.8746 - val_loss: 35279.8860\n",
      "Epoch 826/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 17388.2261 - val_loss: 34882.6263\n",
      "Epoch 827/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 17500.2822 - val_loss: 34119.2324\n",
      "Epoch 828/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 17537.6494 - val_loss: 34502.8613\n",
      "Epoch 829/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 17739.4253 - val_loss: 34257.1758\n",
      "Epoch 830/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 17315.4751 - val_loss: 34303.8493\n",
      "Epoch 831/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 17347.8745 - val_loss: 33531.7856\n",
      "Epoch 832/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 17834.4693 - val_loss: 34076.7677\n",
      "Epoch 833/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 17587.8916 - val_loss: 34973.2157\n",
      "Epoch 834/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 17580.3961 - val_loss: 34623.8122\n",
      "Epoch 835/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 17845.7521 - val_loss: 34878.3151\n",
      "Epoch 836/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 17362.9220 - val_loss: 34767.4708\n",
      "Epoch 837/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 16868.6147 - val_loss: 34249.0763\n",
      "Epoch 838/1000\n",
      "1168/1168 [==============================] - 0s 220us/step - loss: 17427.4236 - val_loss: 34322.8515\n",
      "Epoch 839/1000\n",
      "1168/1168 [==============================] - 0s 137us/step - loss: 17188.7381 - val_loss: 38978.7582\n",
      "Epoch 840/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 18012.0998 - val_loss: 34884.7081\n",
      "Epoch 841/1000\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 17051.6125 - val_loss: 34848.2767\n",
      "Epoch 842/1000\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 18093.0474 - val_loss: 36246.6610\n",
      "Epoch 843/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 16813.1717 - val_loss: 34318.6025\n",
      "Epoch 844/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 16877.2948 - val_loss: 34048.0351\n",
      "Epoch 845/1000\n",
      "1168/1168 [==============================] - 0s 199us/step - loss: 16965.9110 - val_loss: 35575.1434\n",
      "Epoch 846/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 17350.7248 - val_loss: 34209.8632\n",
      "Epoch 847/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 17024.9315 - val_loss: 35381.4733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 848/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 17488.7968 - val_loss: 33985.3176\n",
      "Epoch 849/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 17932.4791 - val_loss: 36686.4956\n",
      "Epoch 850/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 17991.5489 - val_loss: 35674.8968\n",
      "Epoch 851/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 17046.9750 - val_loss: 35568.9511\n",
      "Epoch 852/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 16960.7116 - val_loss: 33987.7481\n",
      "Epoch 853/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 18125.3227 - val_loss: 36500.7057\n",
      "Epoch 854/1000\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 18155.3659 - val_loss: 35285.4380\n",
      "Epoch 855/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 17124.1375 - val_loss: 35085.6405\n",
      "Epoch 856/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 17966.5879 - val_loss: 36033.9312\n",
      "Epoch 857/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 17199.0706 - val_loss: 38406.2890\n",
      "Epoch 858/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 17575.8764 - val_loss: 35017.9158\n",
      "Epoch 859/1000\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 16807.1019 - val_loss: 36494.6525\n",
      "Epoch 860/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 17117.5605 - val_loss: 33709.3865\n",
      "Epoch 861/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 17394.3398 - val_loss: 35340.6137\n",
      "Epoch 862/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 16790.7699 - val_loss: 35201.1507\n",
      "Epoch 863/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 17680.4205 - val_loss: 36011.5312\n",
      "Epoch 864/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 16907.4781 - val_loss: 35268.8203\n",
      "Epoch 865/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 16903.4901 - val_loss: 34438.5084\n",
      "Epoch 866/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 18008.2598 - val_loss: 35028.4376\n",
      "Epoch 867/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 17066.5261 - val_loss: 35019.6649\n",
      "Epoch 868/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 17512.9290 - val_loss: 35545.7984\n",
      "Epoch 869/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 17121.2094 - val_loss: 34583.6133\n",
      "Epoch 870/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 17061.9730 - val_loss: 34620.8738\n",
      "Epoch 871/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 17648.2991 - val_loss: 34842.9696\n",
      "Epoch 872/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 17546.8496 - val_loss: 34432.7963\n",
      "Epoch 873/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 17488.4200 - val_loss: 34002.2355\n",
      "Epoch 874/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 17191.0224 - val_loss: 35904.5970\n",
      "Epoch 875/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 17200.4533 - val_loss: 34111.1454\n",
      "Epoch 876/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 17072.7583 - val_loss: 34098.3377\n",
      "Epoch 877/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 16726.4907 - val_loss: 35588.8824\n",
      "Epoch 878/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 18014.4841 - val_loss: 34127.9958\n",
      "Epoch 879/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 16970.0714 - val_loss: 34041.3251\n",
      "Epoch 880/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 16764.6163 - val_loss: 35359.2492\n",
      "Epoch 881/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 17164.0872 - val_loss: 34476.9322\n",
      "Epoch 882/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 17701.2856 - val_loss: 34339.0502\n",
      "Epoch 883/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 17872.6138 - val_loss: 35013.0006\n",
      "Epoch 884/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 17576.6266 - val_loss: 34932.6045\n",
      "Epoch 885/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 16984.8434 - val_loss: 34630.2039\n",
      "Epoch 886/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 17299.0507 - val_loss: 33780.7222\n",
      "Epoch 887/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 16847.2733 - val_loss: 36247.4292\n",
      "Epoch 888/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 17612.1088 - val_loss: 34062.5628\n",
      "Epoch 889/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 17233.7467 - val_loss: 34814.3840\n",
      "Epoch 890/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 16814.3354 - val_loss: 35198.9904\n",
      "Epoch 891/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 16967.4835 - val_loss: 34576.3972\n",
      "Epoch 892/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 17108.9936 - val_loss: 35598.6571\n",
      "Epoch 893/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 17161.4014 - val_loss: 34925.9637\n",
      "Epoch 894/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 17648.4679 - val_loss: 34631.9336\n",
      "Epoch 895/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 16684.1038 - val_loss: 34132.3303\n",
      "Epoch 896/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 17347.0301 - val_loss: 33928.2295\n",
      "Epoch 897/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 16929.7959 - val_loss: 33637.3099\n",
      "Epoch 898/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 16984.0311 - val_loss: 33652.7426\n",
      "Epoch 899/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 17807.3203 - val_loss: 34214.0360\n",
      "Epoch 900/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 17294.6142 - val_loss: 34507.3977\n",
      "Epoch 901/1000\n",
      "1168/1168 [==============================] - 0s 142us/step - loss: 17621.2434 - val_loss: 34073.5393\n",
      "Epoch 902/1000\n",
      "1168/1168 [==============================] - 0s 210us/step - loss: 17150.5376 - val_loss: 34601.3627\n",
      "Epoch 903/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 16488.6665 - val_loss: 34411.1918\n",
      "Epoch 904/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 16785.7651 - val_loss: 34655.5299\n",
      "Epoch 905/1000\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 17713.7916 - val_loss: 35034.8948\n",
      "Epoch 906/1000\n",
      "1168/1168 [==============================] - 0s 154us/step - loss: 16947.2980 - val_loss: 34926.7362\n",
      "Epoch 907/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 16346.1217 - val_loss: 35637.2662\n",
      "Epoch 908/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 16544.2601 - val_loss: 33990.5294\n",
      "Epoch 909/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 17371.6082 - val_loss: 37500.8703\n",
      "Epoch 910/1000\n",
      "1168/1168 [==============================] - 0s 105us/step - loss: 17347.2739 - val_loss: 34159.4022\n",
      "Epoch 911/1000\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 16724.8403 - val_loss: 36233.7200\n",
      "Epoch 912/1000\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 17409.5666 - val_loss: 36645.5888\n",
      "Epoch 913/1000\n",
      "1168/1168 [==============================] - 0s 105us/step - loss: 17121.6465 - val_loss: 33830.2392\n",
      "Epoch 914/1000\n",
      "1168/1168 [==============================] - 0s 100us/step - loss: 16905.6318 - val_loss: 35293.1047\n",
      "Epoch 915/1000\n",
      "1168/1168 [==============================] - 0s 101us/step - loss: 16992.1080 - val_loss: 34556.3996\n",
      "Epoch 916/1000\n",
      "1168/1168 [==============================] - 0s 107us/step - loss: 16950.1960 - val_loss: 36256.3843\n",
      "Epoch 917/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 16610.3833 - val_loss: 35366.9122\n",
      "Epoch 918/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 17164.3741 - val_loss: 34134.0016\n",
      "Epoch 919/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 16601.5432 - val_loss: 33948.0874\n",
      "Epoch 920/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 16569.2433 - val_loss: 34954.8445\n",
      "Epoch 921/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 16766.2295 - val_loss: 33913.8831\n",
      "Epoch 922/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 17426.9929 - val_loss: 33492.7409\n",
      "Epoch 923/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 17116.0659 - val_loss: 36019.1155\n",
      "Epoch 924/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 16634.6359 - val_loss: 34568.8514\n",
      "Epoch 925/1000\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 16702.7692 - val_loss: 34205.8725\n",
      "Epoch 926/1000\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 16994.2394 - val_loss: 34491.6913\n",
      "Epoch 927/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 16887.3025 - val_loss: 34100.7548\n",
      "Epoch 928/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 17048.0331 - val_loss: 35185.1407\n",
      "Epoch 929/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 16384.4852 - val_loss: 34601.6366\n",
      "Epoch 930/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 16722.6312 - val_loss: 38340.6970\n",
      "Epoch 931/1000\n",
      "1168/1168 [==============================] - 0s 152us/step - loss: 18161.2113 - val_loss: 34092.6068\n",
      "Epoch 932/1000\n",
      "1168/1168 [==============================] - 0s 141us/step - loss: 16659.2042 - val_loss: 34878.4641\n",
      "Epoch 933/1000\n",
      "1168/1168 [==============================] - 0s 141us/step - loss: 16742.1436 - val_loss: 33924.3634\n",
      "Epoch 934/1000\n",
      "1168/1168 [==============================] - 0s 151us/step - loss: 17384.7712 - val_loss: 34175.3496\n",
      "Epoch 935/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 17169.0546 - val_loss: 33848.0837\n",
      "Epoch 936/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 16202.9261 - val_loss: 34012.3542\n",
      "Epoch 937/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 17097.2625 - val_loss: 34056.9209\n",
      "Epoch 938/1000\n",
      "1168/1168 [==============================] - 0s 106us/step - loss: 16779.8364 - val_loss: 33836.6343\n",
      "Epoch 939/1000\n",
      "1168/1168 [==============================] - 0s 102us/step - loss: 18740.6940 - val_loss: 35592.9558\n",
      "Epoch 940/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 16763.8364 - val_loss: 33728.0003\n",
      "Epoch 941/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 17824.6978 - val_loss: 33748.7837\n",
      "Epoch 942/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 17751.3211 - val_loss: 38017.7771\n",
      "Epoch 943/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 16767.0781 - val_loss: 33908.8248\n",
      "Epoch 944/1000\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 16332.7036 - val_loss: 34626.9397\n",
      "Epoch 945/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 16911.0107 - val_loss: 33841.6446\n",
      "Epoch 946/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 16553.0835 - val_loss: 33632.9416\n",
      "Epoch 947/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 17069.6831 - val_loss: 34911.1972\n",
      "Epoch 948/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 17253.2379 - val_loss: 35648.1066\n",
      "Epoch 949/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 16533.0702 - val_loss: 34191.4610\n",
      "Epoch 950/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 16299.2451 - val_loss: 33801.5729\n",
      "Epoch 951/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 16858.3590 - val_loss: 33475.6662\n",
      "Epoch 952/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 16516.6568 - val_loss: 34364.0899\n",
      "Epoch 953/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 17561.1876 - val_loss: 34811.4902\n",
      "Epoch 954/1000\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 16780.2260 - val_loss: 34657.7761\n",
      "Epoch 955/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 16520.3556 - val_loss: 34202.6432\n",
      "Epoch 956/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 17066.6731 - val_loss: 34415.3972\n",
      "Epoch 957/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 17348.6556 - val_loss: 33899.4806\n",
      "Epoch 958/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 16255.4943 - val_loss: 34310.0033\n",
      "Epoch 959/1000\n",
      "1168/1168 [==============================] - 0s 106us/step - loss: 16716.0270 - val_loss: 33475.2073\n",
      "Epoch 960/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 16842.9773 - val_loss: 34693.5844\n",
      "Epoch 961/1000\n",
      "1168/1168 [==============================] - 0s 106us/step - loss: 16884.5097 - val_loss: 34298.9487\n",
      "Epoch 962/1000\n",
      "1168/1168 [==============================] - 0s 107us/step - loss: 17936.4610 - val_loss: 34349.5244\n",
      "Epoch 963/1000\n",
      "1168/1168 [==============================] - 0s 108us/step - loss: 17016.2301 - val_loss: 33988.7476\n",
      "Epoch 964/1000\n",
      "1168/1168 [==============================] - 0s 107us/step - loss: 16827.4749 - val_loss: 33578.5707\n",
      "Epoch 965/1000\n",
      "1168/1168 [==============================] - 0s 106us/step - loss: 16561.2502 - val_loss: 34648.7457\n",
      "Epoch 966/1000\n",
      "1168/1168 [==============================] - 0s 108us/step - loss: 16245.9350 - val_loss: 33776.4671\n",
      "Epoch 967/1000\n",
      "1168/1168 [==============================] - 0s 106us/step - loss: 16627.9136 - val_loss: 36764.9488\n",
      "Epoch 968/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 16301.6878 - val_loss: 34097.5273\n",
      "Epoch 969/1000\n",
      "1168/1168 [==============================] - 0s 106us/step - loss: 16450.4946 - val_loss: 34311.1169\n",
      "Epoch 970/1000\n",
      "1168/1168 [==============================] - 0s 106us/step - loss: 16764.5729 - val_loss: 36905.9158\n",
      "Epoch 971/1000\n",
      "1168/1168 [==============================] - 0s 106us/step - loss: 17053.2160 - val_loss: 34224.0424\n",
      "Epoch 972/1000\n",
      "1168/1168 [==============================] - 0s 108us/step - loss: 16885.6821 - val_loss: 34541.9179\n",
      "Epoch 973/1000\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 17181.8227 - val_loss: 34540.8979\n",
      "Epoch 974/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 16481.2586 - val_loss: 34353.9615\n",
      "Epoch 975/1000\n",
      "1168/1168 [==============================] - 0s 107us/step - loss: 17176.4805 - val_loss: 36402.5462\n",
      "Epoch 976/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 16783.7707 - val_loss: 34173.4971\n",
      "Epoch 977/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 16046.0429 - val_loss: 35478.0720\n",
      "Epoch 978/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 16480.2054 - val_loss: 35044.2535\n",
      "Epoch 979/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 16013.4217 - val_loss: 33880.2966\n",
      "Epoch 980/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 16580.0026 - val_loss: 39393.7111\n",
      "Epoch 981/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 17145.9671 - val_loss: 46150.4823\n",
      "Epoch 982/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 16741.7135 - val_loss: 34384.6437\n",
      "Epoch 983/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 17141.6877 - val_loss: 34361.4296\n",
      "Epoch 984/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 17147.0246 - val_loss: 33573.7949\n",
      "Epoch 985/1000\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 16683.2010 - val_loss: 34768.6753\n",
      "Epoch 986/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 16724.9386 - val_loss: 34927.2756\n",
      "Epoch 987/1000\n",
      "1168/1168 [==============================] - 0s 107us/step - loss: 15477.6847 - val_loss: 33999.6833\n",
      "Epoch 988/1000\n",
      "1168/1168 [==============================] - 0s 108us/step - loss: 16560.4931 - val_loss: 35211.6023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 989/1000\n",
      "1168/1168 [==============================] - 0s 108us/step - loss: 16162.7775 - val_loss: 33766.9999\n",
      "Epoch 990/1000\n",
      "1168/1168 [==============================] - 0s 107us/step - loss: 15978.5032 - val_loss: 35310.7125\n",
      "Epoch 991/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 16433.6512 - val_loss: 35199.7340\n",
      "Epoch 992/1000\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 16247.5633 - val_loss: 34890.5266\n",
      "Epoch 993/1000\n",
      "1168/1168 [==============================] - 0s 106us/step - loss: 16251.1385 - val_loss: 36475.8736\n",
      "Epoch 994/1000\n",
      "1168/1168 [==============================] - 0s 108us/step - loss: 16270.4915 - val_loss: 35104.2526\n",
      "Epoch 995/1000\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 17547.3618 - val_loss: 34526.2764\n",
      "Epoch 996/1000\n",
      "1168/1168 [==============================] - 0s 105us/step - loss: 15963.9581 - val_loss: 34473.7264\n",
      "Epoch 997/1000\n",
      "1168/1168 [==============================] - 0s 107us/step - loss: 16365.3630 - val_loss: 33856.0013\n",
      "Epoch 998/1000\n",
      "1168/1168 [==============================] - 0s 107us/step - loss: 16610.5246 - val_loss: 36860.3520\n",
      "Epoch 999/1000\n",
      "1168/1168 [==============================] - 0s 141us/step - loss: 16808.1144 - val_loss: 33945.3007\n",
      "Epoch 1000/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 15888.8305 - val_loss: 34877.2694\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU,PReLU,ELU\n",
    "from keras.layers import Dropout\n",
    "\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 50, init = 'he_uniform',activation='relu',input_dim = 175))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 25, init = 'he_uniform',activation='relu'))\n",
    "\n",
    "# Adding the third hidden layer\n",
    "classifier.add(Dense(output_dim = 50, init = 'he_uniform',activation='relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'he_uniform'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(loss=root_mean_squared_error, optimizer='Adamax')\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "model_history=classifier.fit(X_train.values, y_train.values,validation_split=0.20, batch_size = 10, nb_epoch = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_pred=classifier.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[123840.62],\n",
       "       [195934.89],\n",
       "       [189745.9 ],\n",
       "       ...,\n",
       "       [167771.83],\n",
       "       [119799.92],\n",
       "       [219374.77]], dtype=float32)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2=pd.DataFrame(ann_pred)\n",
    "sub_df=pd.read_csv('datasets/housing/sample_submission.csv')\n",
    "datasets=pd.concat([sub_df['Id'],pred2],axis=1)\n",
    "datasets.columns=['Id','SalePrice']\n",
    "datasets.to_csv('annResultCJ.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reg(XGBoost with RandomCV) shows the best result among Clf, Reg, Ann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
